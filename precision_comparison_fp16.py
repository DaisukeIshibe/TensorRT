#!/usr/bin/env python3
"""
TensorRT FP32 vs FP16 Precision Comparison
å‡¦ç†é€Ÿåº¦ã¨åˆ†é¡ç²¾åº¦ã®æ¯”è¼ƒï¼ˆINT8ã¯åˆ¥é€”PyTorchã§å®Ÿè£…ï¼‰
"""
import time
import numpy as np
import statistics
import json
import os
from typing import Dict, List, Tuple, Any

def load_test_data():
    """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
    try:
        test_samples = np.load('test_samples.npy')
        test_labels = np.load('test_labels.npy')
        print(f"âœ… Loaded {len(test_samples)} test samples: {test_samples.shape}")
        return test_samples, test_labels
    except FileNotFoundError:
        print("âŒ Test data files not found!")
        return None, None

def calculate_accuracy(predictions: np.ndarray, true_labels: np.ndarray) -> float:
    """åˆ†é¡ç²¾åº¦ã®è¨ˆç®—"""
    predicted_classes = np.argmax(predictions, axis=1)
    true_classes = np.argmax(true_labels, axis=1) if true_labels.ndim > 1 else true_labels
    accuracy = np.mean(predicted_classes == true_classes)
    return accuracy

def benchmark_tensorrt_precision(engine_path: str, test_samples: np.ndarray, 
                                test_labels: np.ndarray, precision_name: str,
                                batch_size: int = 32, num_runs: int = 5) -> Dict[str, Any]:
    """æŒ‡å®šç²¾åº¦ã§ã®TensorRTæ¨è«–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
    print(f"\nğŸ”„ Benchmarking TensorRT {precision_name}...")
    
    try:
        import tensorrt as trt
        import pycuda.driver as cuda
        import pycuda.autoinit
        
        # TensorRT logger
        TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
        
        # ã‚¨ãƒ³ã‚¸ãƒ³èª­ã¿è¾¼ã¿æ™‚é–“æ¸¬å®š
        start_time = time.time()
        runtime = trt.Runtime(TRT_LOGGER)
        with open(engine_path, 'rb') as f:
            engine = runtime.deserialize_cuda_engine(f.read())
        context = engine.create_execution_context()
        load_time = time.time() - start_time
        
        # ãƒ†ãƒ³ã‚½ãƒ«æƒ…å ±å–å¾—
        input_names = []
        output_names = []
        for i in range(engine.num_io_tensors):
            name = engine.get_tensor_name(i)
            if engine.get_tensor_mode(name) == trt.TensorIOMode.INPUT:
                input_names.append(name)
            else:
                output_names.append(name)
        
        input_name = input_names[0]
        output_name = output_names[0]
        
        # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
        warmup_batch = test_samples[:min(batch_size, len(test_samples))]
        context.set_input_shape(input_name, [len(warmup_batch), 32, 32, 3])
        warmup_output_shape = context.get_tensor_shape(output_name)
        warmup_input_data = warmup_batch.astype(np.float32)
        warmup_output_data = np.empty(warmup_output_shape, dtype=np.float32)
        
        warmup_input_gpu = cuda.mem_alloc(warmup_input_data.nbytes)
        warmup_output_gpu = cuda.mem_alloc(warmup_output_data.nbytes)
        cuda.memcpy_htod(warmup_input_gpu, warmup_input_data)
        context.set_tensor_address(input_name, warmup_input_gpu)
        context.set_tensor_address(output_name, warmup_output_gpu)
        context.execute_async_v3(0)
        cuda.memcpy_dtoh(warmup_output_data, warmup_output_gpu)
        warmup_input_gpu.free()
        warmup_output_gpu.free()
        
        # ãƒãƒƒãƒå‡¦ç†ã§æ¸¬å®š
        num_batches = (len(test_samples) + batch_size - 1) // batch_size
        all_predictions = []
        inference_times = []
        
        for run in range(num_runs):
            run_predictions = []
            start_time = time.time()
            
            for batch_idx in range(num_batches):
                start_idx = batch_idx * batch_size
                end_idx = min(start_idx + batch_size, len(test_samples))
                current_batch_size = end_idx - start_idx
                
                batch_data = test_samples[start_idx:end_idx]
                
                # å…¥åŠ›å½¢çŠ¶è¨­å®š
                context.set_input_shape(input_name, [current_batch_size, 32, 32, 3])
                output_shape = context.get_tensor_shape(output_name)
                
                # ãƒ‡ãƒ¼ã‚¿æº–å‚™
                input_data = batch_data.astype(np.float32)
                output_data = np.empty(output_shape, dtype=np.float32)
                
                # GPU ãƒ¡ãƒ¢ãƒªç¢ºä¿
                input_gpu = cuda.mem_alloc(input_data.nbytes)
                output_gpu = cuda.mem_alloc(output_data.nbytes)
                
                # ãƒ‡ãƒ¼ã‚¿è»¢é€ãƒ»æ¨è«–ãƒ»çµæœå–å¾—
                cuda.memcpy_htod(input_gpu, input_data)
                context.set_tensor_address(input_name, input_gpu)
                context.set_tensor_address(output_name, output_gpu)
                context.execute_async_v3(0)
                cuda.memcpy_dtoh(output_data, output_gpu)
                
                # GPU ãƒ¡ãƒ¢ãƒªè§£æ”¾
                input_gpu.free()
                output_gpu.free()
                
                run_predictions.append(output_data)
            
            inference_time = time.time() - start_time
            inference_times.append(inference_time)
            
            if run == 0:  # æœ€åˆã®å®Ÿè¡Œçµæœã‚’ç²¾åº¦è¨ˆç®—ç”¨ã«ä¿å­˜
                all_predictions = np.vstack(run_predictions)
            
            print(f"  Run {run+1}: {inference_time:.4f}s ({num_batches} batches)")
        
        # ç²¾åº¦è¨ˆç®—
        accuracy = calculate_accuracy(all_predictions, test_labels)
        
        result = {
            'precision': precision_name,
            'load_time': load_time,
            'inference_times': inference_times,
            'mean_time': statistics.mean(inference_times),
            'std_time': statistics.stdev(inference_times),
            'samples_per_second': len(test_samples) / statistics.mean(inference_times),
            'accuracy': accuracy,
            'num_batches': num_batches,
            'batch_size': batch_size
        }
        
        print(f"âœ… {precision_name} - Load: {load_time:.3f}s, Inference: {result['mean_time']:.4f}s Â± {result['std_time']:.4f}s")
        print(f"   Throughput: {result['samples_per_second']:.1f} samples/sec, Accuracy: {accuracy:.4f}")
        
        return result
        
    except Exception as e:
        print(f"âŒ Error benchmarking {precision_name}: {e}")
        return None

def generate_fp16_engine():
    """FP16ã®TensorRTã‚¨ãƒ³ã‚¸ãƒ³ã‚’ç”Ÿæˆ"""
    print("ğŸ”§ Generating TensorRT FP16 engine...")
    
    try:
        import tensorrt as trt
        
        # TensorRT logger
        TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
        
        # ONNXãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
        onnx_model_path = 'model.onnx'
        if not os.path.exists(onnx_model_path):
            print("âŒ ONNX model not found! Please convert the model first.")
            return False
        
        builder = trt.Builder(TRT_LOGGER)
        
        # FP16ã‚¨ãƒ³ã‚¸ãƒ³ç”Ÿæˆ
        print("\nğŸ“¦ Building FP16 engine...")
        network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
        parser = trt.OnnxParser(network, TRT_LOGGER)
        
        with open(onnx_model_path, 'rb') as model:
            if not parser.parse(model.read()):
                print("âŒ Failed to parse ONNX model for FP16")
                for error in range(parser.num_errors):
                    print(f"   {parser.get_error(error)}")
                return False
        
        config = builder.create_builder_config()
        
        # FP16è¨­å®š
        if builder.platform_has_fast_fp16:
            config.set_flag(trt.BuilderFlag.FP16)
            print("âœ… FP16 precision enabled")
        else:
            print("âš ï¸ FP16 not supported, using FP32")
        
        # æœ€é©åŒ–ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®š
        profile = builder.create_optimization_profile()
        profile.set_shape('input_1', [1, 32, 32, 3], [16, 32, 32, 3], [32, 32, 32, 3])
        config.add_optimization_profile(profile)
        
        # ã‚¨ãƒ³ã‚¸ãƒ³ãƒ“ãƒ«ãƒ‰
        serialized_engine = builder.build_serialized_network(network, config)
        if serialized_engine:
            with open('model_fp16.trt', 'wb') as f:
                f.write(serialized_engine)
            print("âœ… FP16 engine saved as model_fp16.trt")
            return True
        else:
            print("âŒ Failed to build FP16 engine")
            return False
        
    except Exception as e:
        print(f"âŒ Error generating FP16 engine: {e}")
        return False

def benchmark_pytorch_quantized():
    """PyTorchã§ã®INT8é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
    print("\nğŸ”„ Benchmarking PyTorch INT8 Quantization...")
    
    try:
        # TensorFlowã‚³ãƒ³ãƒ†ãƒŠã§ã‚‚åˆ©ç”¨å¯èƒ½ãªtorchï¼ˆåˆ¥é€”å®Ÿè£…ãŒå¿…è¦ï¼‰
        print("âš ï¸ PyTorch INT8 quantization requires separate implementation")
        print("   This would need PyTorch with quantization support")
        
        # ä»®ã®çµæœï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯é©åˆ‡ãªé‡å­åŒ–ã‚’è¡Œã†ï¼‰
        mock_result = {
            'precision': 'INT8 (PyTorch)',
            'load_time': 0.2,
            'mean_time': 0.008,
            'std_time': 0.001,
            'samples_per_second': 12500.0,
            'accuracy': 0.78,  # é‡å­åŒ–ã«ã‚ˆã‚‹ç²¾åº¦ä½ä¸‹ã‚’æƒ³å®š
            'num_batches': 4,
            'batch_size': 32,
            'note': 'Mock result - requires PyTorch implementation'
        }
        
        return mock_result
        
    except Exception as e:
        print(f"âŒ Error with PyTorch quantization: {e}")
        return None

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ TensorRT Precision Comparison: FP32 vs FP16")
    print("=" * 50)
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    test_samples, test_labels = load_test_data()
    if test_samples is None:
        return
    
    # æ—¢å­˜ã®FP32ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½¿ç”¨
    fp32_engine_path = 'model.trt'
    if not os.path.exists(fp32_engine_path):
        print("âŒ FP32 TensorRT engine not found!")
        return
    
    # FP16ã‚¨ãƒ³ã‚¸ãƒ³ç”Ÿæˆ
    if not generate_fp16_engine():
        print("âŒ Failed to generate FP16 engine")
        return
    
    # FP32ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
    print("\n" + "="*50)
    fp32_result = benchmark_tensorrt_precision(
        fp32_engine_path, test_samples, test_labels, 'FP32'
    )
    
    # FP16ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
    fp16_result = benchmark_tensorrt_precision(
        'model_fp16.trt', test_samples, test_labels, 'FP16'
    )
    
    # PyTorch INT8æ¯”è¼ƒï¼ˆãƒ¢ãƒƒã‚¯ï¼‰
    int8_result = benchmark_pytorch_quantized()
    
    # çµæœæ¯”è¼ƒ
    if fp32_result and fp16_result:
        print("\nğŸ“Š Precision Comparison Results:")
        print("=" * 80)
        print(f"{'Metric':<20} {'FP32':<15} {'FP16':<15} {'Speedup (FP16/FP32)':<20}")
        print("-" * 80)
        
        # æ¨è«–æ™‚é–“æ¯”è¼ƒ
        fp32_time = fp32_result['mean_time']
        fp16_time = fp16_result['mean_time']
        time_speedup = fp32_time / fp16_time
        print(f"{'Inference Time':<20} {fp32_time:.4f}s {fp16_time:.4f}s {time_speedup:.2f}x")
        
        # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ¯”è¼ƒ
        fp32_throughput = fp32_result['samples_per_second']
        fp16_throughput = fp16_result['samples_per_second']
        throughput_speedup = fp16_throughput / fp32_throughput
        print(f"{'Throughput':<20} {fp32_throughput:.1f}/s {fp16_throughput:.1f}/s {throughput_speedup:.2f}x")
        
        # ç²¾åº¦æ¯”è¼ƒ
        fp32_accuracy = fp32_result['accuracy']
        fp16_accuracy = fp16_result['accuracy']
        accuracy_diff = fp16_accuracy - fp32_accuracy
        print(f"{'Accuracy':<20} {fp32_accuracy:.4f} {fp16_accuracy:.4f} {accuracy_diff:+.4f}")
        
        # ãƒ­ãƒ¼ãƒ‰æ™‚é–“æ¯”è¼ƒ
        fp32_load = fp32_result['load_time']
        fp16_load = fp16_result['load_time']
        load_speedup = fp32_load / fp16_load
        print(f"{'Load Time':<20} {fp32_load:.3f}s {fp16_load:.3f}s {load_speedup:.2f}x")
        
        # ã‚¨ãƒ³ã‚¸ãƒ³ã‚µã‚¤ã‚ºæ¯”è¼ƒ
        fp32_size = os.path.getsize(fp32_engine_path) / (1024*1024)
        fp16_size = os.path.getsize('model_fp16.trt') / (1024*1024)
        size_reduction = fp32_size / fp16_size
        print(f"{'Engine Size':<20} {fp32_size:.1f}MB {fp16_size:.1f}MB {size_reduction:.2f}x smaller")
        
        print("\nğŸ¯ FP32 vs FP16 Summary:")
        print(f"âœ… FP16 is {throughput_speedup:.1f}x faster than FP32")
        print(f"âœ… FP16 engine is {size_reduction:.1f}x smaller than FP32")
        if abs(accuracy_diff) < 0.01:
            print(f"âœ… Accuracy difference is minimal: {accuracy_diff:+.4f}")
        else:
            print(f"âš ï¸ Accuracy difference: {accuracy_diff:+.4f}")
        
        # INT8çµæœã‚‚è¡¨ç¤ºï¼ˆãƒ¢ãƒƒã‚¯ï¼‰
        if int8_result:
            print(f"\nğŸ“‹ INT8 Reference (Mock):")
            print(f"   Throughput: {int8_result['samples_per_second']:.1f} samples/sec")
            print(f"   Accuracy: {int8_result['accuracy']:.4f}")
            print(f"   Note: {int8_result['note']}")
        
        # çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        results = {
            'fp32': fp32_result,
            'fp16': fp16_result,
            'int8_mock': int8_result,
            'comparison': {
                'fp16_speedup': throughput_speedup,
                'accuracy_difference': accuracy_diff,
                'load_time_speedup': load_speedup,
                'size_reduction': size_reduction
            }
        }
        
        with open('precision_comparison_results.json', 'w') as f:
            json.dump(results, f, indent=2)
        print(f"\nğŸ’¾ Results saved to precision_comparison_results.json")

if __name__ == "__main__":
    main()