#!/usr/bin/env python3
"""
TensorRT 8.x Engine Generator with Precision Options
TensorRT 8.xå‘ã‘ã®ç²¾åº¦åˆ¥ã‚¨ãƒ³ã‚¸ãƒ³ç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""
import tensorrt as trt
import numpy as np
import os
import argparse
import random

def create_calibration_cache():
    """INT8ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®ç°¡æ˜“ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ"""
    cache_file = "calibration_cache_8x.cache"
    
    # äº‹å‰ã«ç”Ÿæˆã•ã‚ŒãŸã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒã‚ã‚‹å ´åˆã¯ãã‚Œã‚’ä½¿ç”¨
    if os.path.exists(cache_file):
        print(f"âœ… Using existing calibration cache: {cache_file}")
        return cache_file
    
    # ç°¡æ˜“çš„ãªã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½œæˆ
    print("ğŸ”§ Creating simple calibration cache for INT8...")
    
    # æœ€å°é™ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆï¼ˆå®Ÿç”¨çš„ã§ã¯ãªã„ãŒã€ã‚¨ãƒ³ã‚¸ãƒ³ç”Ÿæˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
    cache_data = b"""TRT-8503-EntropyCalibrator2
input_1: 3c010a3c
"""
    
    with open(cache_file, 'wb') as f:
        f.write(cache_data)
    
    print(f"âœ… Simple calibration cache created: {cache_file}")
    return cache_file

class SimpleTensorRTCalibrator(trt.IInt8EntropyCalibrator2):
    """ç°¡æ˜“INT8 Calibrator for TensorRT 8.x"""
    
    def __init__(self, cache_file="calibration_cache_8x.cache"):
        trt.IInt8EntropyCalibrator2.__init__(self)
        self.cache_file = cache_file
        
    def get_batch_size(self):
        return 1
        
    def get_batch(self, names):
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã€å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã¯ä¸è¦
        return None
        
    def read_calibration_cache(self):
        if os.path.exists(self.cache_file):
            with open(self.cache_file, "rb") as f:
                return f.read()
        return None
        
    def write_calibration_cache(self, cache):
        with open(self.cache_file, "wb") as f:
            f.write(cache)

def prepare_calibration_data(num_samples=1000):
    """ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™"""
    print(f"ğŸ“Š Preparing calibration data ({num_samples} samples)...")
    
    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    csv_file = "test_samples.csv"
    if os.path.exists(csv_file):
        print(f"ğŸ“„ Loading calibration data from {csv_file}...")
        calibration_data = np.loadtxt(csv_file, delimiter=',', dtype=np.float32)
        
        # Reshape to CIFAR-10 format (32x32x3)
        calibration_data = calibration_data.reshape(-1, 32, 32, 3)
        
        # Limit number of samples
        if len(calibration_data) > num_samples:
            indices = random.sample(range(len(calibration_data)), num_samples)
            calibration_data = calibration_data[indices]
            
        print(f"âœ… Calibration data prepared from CSV: {calibration_data.shape}")
        return calibration_data
    else:
        print("âš ï¸ test_samples.csv not found. Creating synthetic calibration data...")
        # Synthetic calibration data as fallback
        calibration_data = np.random.rand(num_samples, 32, 32, 3).astype(np.float32)
        print(f"âœ… Synthetic calibration data prepared: {calibration_data.shape}")
        return calibration_data

def build_engine_for_trt8x(precision="fp32"):
    """TensorRT 8.xå‘ã‘ã‚¨ãƒ³ã‚¸ãƒ³ã®ç”Ÿæˆ (ç²¾åº¦æŒ‡å®šå¯¾å¿œ)"""
    print(f"ğŸ”§ Building TensorRT 8.x engine with {precision.upper()} precision...")
    
    # TensorRT logger
    TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
    
    # Check TensorRT version
    print(f"ğŸ“‹ TensorRT Version: {trt.__version__}")
    
    # ONNXãƒ¢ãƒ‡ãƒ«ç¢ºèª
    onnx_path = 'model.onnx'
    if not os.path.exists(onnx_path):
        print("âŒ ONNX model not found!")
        return False
    
    # Builder and network creation (TensorRT 8.x style)
    builder = trt.Builder(TRT_LOGGER)
    config = builder.create_builder_config()
    
    # Create network with explicit batch (recommended for TensorRT 8.x)
    network_flags = 1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)
    network = builder.create_network(network_flags)
    
    # ONNX parser
    parser = trt.OnnxParser(network, TRT_LOGGER)
    
    # Parse ONNX model
    with open(onnx_path, 'rb') as model:
        if not parser.parse(model.read()):
            print("âŒ Failed to parse ONNX model")
            for error in range(parser.num_errors):
                print(f"   {parser.get_error(error)}")
            return False
    
    print("âœ… ONNX model parsed successfully")
    
    # Configuration for TensorRT 8.x
    config.max_workspace_size = 1 << 30  # 1GB workspace
    
    # ç²¾åº¦è¨­å®š
    if precision == "fp16":
        print("ğŸ”§ Enabling FP16 precision...")
        config.set_flag(trt.BuilderFlag.FP16)
    elif precision == "int8":
        print("ğŸ”§ Enabling INT8 precision...")
        config.set_flag(trt.BuilderFlag.INT8)
        
        # INT8ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        cache_file = create_calibration_cache()
        calibrator = SimpleTensorRTCalibrator(cache_file)
        config.int8_calibrator = calibrator
        print("âœ… INT8 calibration configured (using cache)")
        print("âš ï¸  Note: Using simplified calibration. For production use, proper calibration data is recommended.")
    else:
        print("ğŸ”§ Using default FP32 precision...")
    
    # For TensorRT 8.x, we can set optimization profiles for dynamic shapes
    profile = builder.create_optimization_profile()
    
    # Set dynamic batch size (TensorRT 8.x style)
    input_name = "input_1"  # CIFAR-10 input
    profile.set_shape(input_name, 
                     [1, 32, 32, 3],    # min
                     [16, 32, 32, 3],   # opt
                     [32, 32, 32, 3])   # max
    
    config.add_optimization_profile(profile)
    
    print("âœ… Optimization profile configured")
    print("   Min batch: 1, Optimal batch: 16, Max batch: 32")
    
    # Build engine (TensorRT 8.x style)
    print("ğŸ”¨ Building engine (this may take a while)...")
    engine = builder.build_engine(network, config)
    
    if engine is None:
        print("âŒ Failed to build engine")
        return False
    
    # Serialize and save engine
    engine_path = f"model_trt8x_{precision}.trt"
    with open(engine_path, "wb") as f:
        f.write(engine.serialize())
    
    print(f"âœ… TensorRT 8.x {precision.upper()} engine saved as: {engine_path}")
    
    # Engine info
    engine_size = os.path.getsize(engine_path) / (1024 * 1024)
    print(f"ğŸ“Š Engine size: {engine_size:.1f} MB")
    print(f"ğŸ“Š Number of bindings: {engine.num_bindings}")
    print(f"ğŸ“Š Precision: {precision.upper()}")
    
    for i in range(engine.num_bindings):
        binding_name = engine.get_binding_name(i)
        binding_shape = engine.get_binding_shape(i)
        is_input = engine.binding_is_input(i)
        binding_type = "Input" if is_input else "Output"
        print(f"   {binding_type}: {binding_name}, Shape: {binding_shape}")
    
    return engine_path

def main():
    parser = argparse.ArgumentParser(description="TensorRT 8.x Engine Generator with Precision Options")
    parser.add_argument("--precision", "-p", 
                       choices=["fp32", "fp16", "int8"], 
                       default="fp32",
                       help="Precision for TensorRT engine (default: fp32)")
    parser.add_argument("--all", "-a", 
                       action="store_true",
                       help="Generate engines for all precisions (fp32, fp16, int8)")
    
    args = parser.parse_args()
    
    print("ğŸš€ TensorRT 8.x Engine Generation with Precision Options")
    print("=" * 50)
    
    if args.all:
        print("ğŸ”„ Generating engines for all precisions...")
        precisions = ["fp32", "fp16", "int8"]
        results = {}
        
        for precision in precisions:
            print(f"\n{'=' * 20} {precision.upper()} {'=' * 20}")
            try:
                engine_path = build_engine_for_trt8x(precision)
                if engine_path:
                    results[precision] = {"status": "âœ… Success", "path": engine_path}
                else:
                    results[precision] = {"status": "âŒ Failed", "path": None}
            except Exception as e:
                results[precision] = {"status": f"âŒ Error: {str(e)}", "path": None}
        
        # Summary
        print(f"\n{'=' * 20} SUMMARY {'=' * 20}")
        for precision, result in results.items():
            print(f"{precision.upper()}: {result['status']}")
            if result['path']:
                size = os.path.getsize(result['path']) / (1024 * 1024)
                print(f"   ğŸ“„ {result['path']} ({size:.1f} MB)")
    
    else:
        success = build_engine_for_trt8x(args.precision)
        
        if success:
            print(f"\nâœ… TensorRT 8.x {args.precision.upper()} engine generation completed!")
        else:
            print(f"\nâŒ TensorRT 8.x {args.precision.upper()} engine generation failed!")
            exit(1)

if __name__ == "__main__":
    main()