#!/usr/bin/env python3
"""
Performance Benchmark: SavedModel vs ONNX vs TensorRT (Batch-optimized)
å„ãƒ¢ãƒ‡ãƒ«å½¢å¼ã§ã®æ¨è«–é€Ÿåº¦ã‚’è©³ç´°ã«æ¯”è¼ƒæ¸¬å®š
"""
import time
import numpy as np
import os
import statistics
from typing import List, Dict, Any

def load_test_data(num_samples: int = 100):
    """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
    try:
        test_samples = np.load('test_samples.npy')[:num_samples]
        test_labels = np.load('test_labels.npy')[:num_samples]
        print(f"âœ… Loaded {len(test_samples)} test samples: {test_samples.shape}")
        return test_samples, test_labels
    except FileNotFoundError:
        print("âŒ Test data not found. Please run data generation first.")
        return None, None

def benchmark_savedmodel(test_samples: np.ndarray, num_runs: int = 5) -> Dict[str, Any]:
    """SavedModelæ¨è«–é€Ÿåº¦æ¸¬å®š"""
    print("\nğŸ”„ Benchmarking SavedModel...")
    
    try:
        import tensorflow as tf
        
        # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿æ™‚é–“æ¸¬å®š
        start_time = time.time()
        model = tf.keras.models.load_model('cifar10_vgg_model')
        load_time = time.time() - start_time
        
        # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
        _ = model.predict(test_samples[:1], verbose=0)
        
        # è¤‡æ•°å›æ¸¬å®š
        inference_times = []
        for run in range(num_runs):
            start_time = time.time()
            predictions = model.predict(test_samples, verbose=0)
            inference_time = time.time() - start_time
            inference_times.append(inference_time)
            print(f"  Run {run+1}: {inference_time:.4f}s")
        
        return {
            'name': 'SavedModel',
            'load_time': load_time,
            'inference_times': inference_times,
            'mean_time': statistics.mean(inference_times),
            'std_time': statistics.stdev(inference_times) if len(inference_times) > 1 else 0,
            'samples_per_second': len(test_samples) / statistics.mean(inference_times),
            'status': 'âœ… Success'
        }
    except Exception as e:
        print(f"âŒ SavedModel benchmark failed: {e}")
        return {'name': 'SavedModel', 'status': f'âŒ Failed: {e}'}

def benchmark_onnx(test_samples: np.ndarray, num_runs: int = 5) -> Dict[str, Any]:
    """ONNXæ¨è«–é€Ÿåº¦æ¸¬å®š"""
    print("\nğŸ”„ Benchmarking ONNX...")
    
    try:
        import onnxruntime as ort
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆæ™‚é–“æ¸¬å®š
        start_time = time.time()
        session = ort.InferenceSession('model.onnx', providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])
        load_time = time.time() - start_time
        
        input_name = session.get_inputs()[0].name
        ort_inputs = {input_name: test_samples.astype(np.float32)}
        
        # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
        _ = session.run(None, {input_name: test_samples[:1].astype(np.float32)})
        
        # è¤‡æ•°å›æ¸¬å®š
        inference_times = []
        for run in range(num_runs):
            start_time = time.time()
            predictions = session.run(None, ort_inputs)
            inference_time = time.time() - start_time
            inference_times.append(inference_time)
            print(f"  Run {run+1}: {inference_time:.4f}s")
        
        return {
            'name': 'ONNX',
            'load_time': load_time,
            'inference_times': inference_times,
            'mean_time': statistics.mean(inference_times),
            'std_time': statistics.stdev(inference_times) if len(inference_times) > 1 else 0,
            'samples_per_second': len(test_samples) / statistics.mean(inference_times),
            'status': 'âœ… Success'
        }
    except Exception as e:
        print(f"âŒ ONNX benchmark failed: {e}")
        return {'name': 'ONNX', 'status': f'âŒ Failed: {e}'}

def benchmark_tensorrt_batch(test_samples: np.ndarray, batch_size: int = 32, num_runs: int = 5) -> Dict[str, Any]:
    """TensorRT ãƒãƒƒãƒå‡¦ç†æ¨è«–é€Ÿåº¦æ¸¬å®š"""
    print(f"\nğŸ”„ Benchmarking TensorRT (Batch size: {batch_size})...")
    
    try:
        import tensorrt as trt
        import pycuda.driver as cuda
        import pycuda.autoinit
        
        # TensorRT logger
        TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
        
        # ã‚¨ãƒ³ã‚¸ãƒ³èª­ã¿è¾¼ã¿æ™‚é–“æ¸¬å®š
        start_time = time.time()
        runtime = trt.Runtime(TRT_LOGGER)
        with open('model.trt', 'rb') as f:
            engine = runtime.deserialize_cuda_engine(f.read())
        context = engine.create_execution_context()
        load_time = time.time() - start_time
        
        # ãƒ†ãƒ³ã‚½ãƒ«æƒ…å ±å–å¾—
        input_names = []
        output_names = []
        for i in range(engine.num_io_tensors):
            name = engine.get_tensor_name(i)
            if engine.get_tensor_mode(name) == trt.TensorIOMode.INPUT:
                input_names.append(name)
            else:
                output_names.append(name)
        
        input_name = input_names[0]
        output_name = output_names[0]
        
        # ãƒãƒƒãƒå‡¦ç†
        num_batches = (len(test_samples) + batch_size - 1) // batch_size
        
        # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
        warmup_batch = test_samples[:min(batch_size, len(test_samples))]
        context.set_input_shape(input_name, [len(warmup_batch), 32, 32, 3])
        warmup_output_shape = context.get_tensor_shape(output_name)
        warmup_input_data = warmup_batch.astype(np.float32)
        warmup_output_data = np.empty(warmup_output_shape, dtype=np.float32)
        
        warmup_input_gpu = cuda.mem_alloc(warmup_input_data.nbytes)
        warmup_output_gpu = cuda.mem_alloc(warmup_output_data.nbytes)
        cuda.memcpy_htod(warmup_input_gpu, warmup_input_data)
        context.set_tensor_address(input_name, warmup_input_gpu)
        context.set_tensor_address(output_name, warmup_output_gpu)
        context.execute_async_v3(0)
        cuda.memcpy_dtoh(warmup_output_data, warmup_output_gpu)
        warmup_input_gpu.free()
        warmup_output_gpu.free()
        
        # è¤‡æ•°å›æ¸¬å®š
        inference_times = []
        for run in range(num_runs):
            start_time = time.time()
            
            for batch_idx in range(num_batches):
                start_idx = batch_idx * batch_size
                end_idx = min(start_idx + batch_size, len(test_samples))
                current_batch_size = end_idx - start_idx
                
                batch_data = test_samples[start_idx:end_idx]
                
                # å…¥åŠ›å½¢çŠ¶è¨­å®š
                context.set_input_shape(input_name, [current_batch_size, 32, 32, 3])
                output_shape = context.get_tensor_shape(output_name)
                
                # ãƒ‡ãƒ¼ã‚¿æº–å‚™
                input_data = batch_data.astype(np.float32)
                output_data = np.empty(output_shape, dtype=np.float32)
                
                # GPU ãƒ¡ãƒ¢ãƒªç¢ºä¿
                input_gpu = cuda.mem_alloc(input_data.nbytes)
                output_gpu = cuda.mem_alloc(output_data.nbytes)
                
                # ãƒ‡ãƒ¼ã‚¿è»¢é€ãƒ»æ¨è«–ãƒ»çµæœå–å¾—
                cuda.memcpy_htod(input_gpu, input_data)
                context.set_tensor_address(input_name, input_gpu)
                context.set_tensor_address(output_name, output_gpu)
                context.execute_async_v3(0)
                cuda.memcpy_dtoh(output_data, output_gpu)
                
                # GPU ãƒ¡ãƒ¢ãƒªè§£æ”¾
                input_gpu.free()
                output_gpu.free()
            
            inference_time = time.time() - start_time
            inference_times.append(inference_time)
            print(f"  Run {run+1}: {inference_time:.4f}s ({num_batches} batches)")
        
        return {
            'name': f'TensorRT (Batch {batch_size})',
            'load_time': load_time,
            'inference_times': inference_times,
            'mean_time': statistics.mean(inference_times),
            'std_time': statistics.stdev(inference_times) if len(inference_times) > 1 else 0,
            'samples_per_second': len(test_samples) / statistics.mean(inference_times),
            'batch_size': batch_size,
            'num_batches': num_batches,
            'status': 'âœ… Success'
        }
    except Exception as e:
        print(f"âŒ TensorRT benchmark failed: {e}")
        import traceback
        traceback.print_exc()
        return {'name': f'TensorRT (Batch {batch_size})', 'status': f'âŒ Failed: {e}'}

def benchmark_tensorrt_single(test_samples: np.ndarray, num_runs: int = 5) -> Dict[str, Any]:
    """TensorRT å˜ä¸€ã‚µãƒ³ãƒ—ãƒ«å‡¦ç†æ¨è«–é€Ÿåº¦æ¸¬å®šï¼ˆæ¯”è¼ƒç”¨ï¼‰"""
    print(f"\nğŸ”„ Benchmarking TensorRT (Single sample processing)...")
    
    try:
        import tensorrt as trt
        import pycuda.driver as cuda
        import pycuda.autoinit
        
        # TensorRT logger
        TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
        
        # ã‚¨ãƒ³ã‚¸ãƒ³èª­ã¿è¾¼ã¿
        start_time = time.time()
        runtime = trt.Runtime(TRT_LOGGER)
        with open('model.trt', 'rb') as f:
            engine = runtime.deserialize_cuda_engine(f.read())
        context = engine.create_execution_context()
        load_time = time.time() - start_time
        
        # ãƒ†ãƒ³ã‚½ãƒ«æƒ…å ±å–å¾—
        input_names = []
        output_names = []
        for i in range(engine.num_io_tensors):
            name = engine.get_tensor_name(i)
            if engine.get_tensor_mode(name) == trt.TensorIOMode.INPUT:
                input_names.append(name)
            else:
                output_names.append(name)
        
        input_name = input_names[0]
        output_name = output_names[0]
        
        # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
        context.set_input_shape(input_name, [1, 32, 32, 3])
        warmup_output_shape = context.get_tensor_shape(output_name)
        warmup_input_data = test_samples[:1].astype(np.float32)
        warmup_output_data = np.empty(warmup_output_shape, dtype=np.float32)
        
        warmup_input_gpu = cuda.mem_alloc(warmup_input_data.nbytes)
        warmup_output_gpu = cuda.mem_alloc(warmup_output_data.nbytes)
        cuda.memcpy_htod(warmup_input_gpu, warmup_input_data)
        context.set_tensor_address(input_name, warmup_input_gpu)
        context.set_tensor_address(output_name, warmup_output_gpu)
        context.execute_async_v3(0)
        cuda.memcpy_dtoh(warmup_output_data, warmup_output_gpu)
        warmup_input_gpu.free()
        warmup_output_gpu.free()
        
        # è¤‡æ•°å›æ¸¬å®šï¼ˆæœ€åˆã®10ã‚µãƒ³ãƒ—ãƒ«ã®ã¿ï¼‰
        limited_samples = test_samples[:10]  # æ™‚é–“çŸ­ç¸®ã®ãŸã‚10ã‚µãƒ³ãƒ—ãƒ«ã®ã¿
        inference_times = []
        for run in range(num_runs):
            start_time = time.time()
            
            for i, sample in enumerate(limited_samples):
                # å…¥åŠ›å½¢çŠ¶è¨­å®š
                context.set_input_shape(input_name, [1, 32, 32, 3])
                output_shape = context.get_tensor_shape(output_name)
                
                # ãƒ‡ãƒ¼ã‚¿æº–å‚™
                input_data = sample.reshape(1, 32, 32, 3).astype(np.float32)
                output_data = np.empty(output_shape, dtype=np.float32)
                
                # GPU ãƒ¡ãƒ¢ãƒªç¢ºä¿
                input_gpu = cuda.mem_alloc(input_data.nbytes)
                output_gpu = cuda.mem_alloc(output_data.nbytes)
                
                # ãƒ‡ãƒ¼ã‚¿è»¢é€ãƒ»æ¨è«–ãƒ»çµæœå–å¾—
                cuda.memcpy_htod(input_gpu, input_data)
                context.set_tensor_address(input_name, input_gpu)
                context.set_tensor_address(output_name, output_gpu)
                context.execute_async_v3(0)
                cuda.memcpy_dtoh(output_data, output_gpu)
                
                # GPU ãƒ¡ãƒ¢ãƒªè§£æ”¾
                input_gpu.free()
                output_gpu.free()
            
            inference_time = time.time() - start_time
            inference_times.append(inference_time)
            print(f"  Run {run+1}: {inference_time:.4f}s ({len(limited_samples)} samples)")
        
        # 100ã‚µãƒ³ãƒ—ãƒ«ç›¸å½“ã«æ›ç®—
        scaled_time = statistics.mean(inference_times) * (len(test_samples) / len(limited_samples))
        
        return {
            'name': 'TensorRT (Single)',
            'load_time': load_time,
            'inference_times': inference_times,
            'mean_time': statistics.mean(inference_times),
            'scaled_time_100': scaled_time,
            'std_time': statistics.stdev(inference_times) if len(inference_times) > 1 else 0,
            'samples_per_second': len(limited_samples) / statistics.mean(inference_times),
            'scaled_samples_per_second': len(test_samples) / scaled_time,
            'samples_tested': len(limited_samples),
            'status': 'âœ… Success'
        }
    except Exception as e:
        print(f"âŒ TensorRT Single benchmark failed: {e}")
        import traceback
        traceback.print_exc()
        return {'name': 'TensorRT (Single)', 'status': f'âŒ Failed: {e}'}

def print_benchmark_results(results: List[Dict[str, Any]]):
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã®è¡¨ç¤º"""
    print("\n" + "="*80)
    print("ğŸ BENCHMARK RESULTS SUMMARY")
    print("="*80)
    
    # æˆåŠŸã—ãŸçµæœã®ã¿ãƒ•ã‚£ãƒ«ã‚¿
    successful_results = [r for r in results if 'âœ… Success' in r.get('status', '')]
    
    if not successful_results:
        print("âŒ No successful benchmarks to compare")
        return
    
    # çµæœãƒ†ãƒ¼ãƒ–ãƒ«
    print(f"{'Model':<25} {'Load Time':<12} {'Avg Inference':<15} {'Std Dev':<10} {'Samples/sec':<12} {'Status'}")
    print("-" * 80)
    
    for result in results:
        if 'âœ… Success' in result.get('status', ''):
            load_time = f"{result['load_time']:.3f}s"
            if 'scaled_time_100' in result:
                avg_time = f"{result['scaled_time_100']:.3f}s*"
                samples_per_sec = f"{result['scaled_samples_per_second']:.1f}*"
            else:
                avg_time = f"{result['mean_time']:.3f}s"
                samples_per_sec = f"{result['samples_per_second']:.1f}"
            std_time = f"{result['std_time']:.3f}s"
            
            print(f"{result['name']:<25} {load_time:<12} {avg_time:<15} {std_time:<10} {samples_per_sec:<12} {result['status']}")
        else:
            print(f"{result['name']:<25} {'N/A':<12} {'N/A':<15} {'N/A':<10} {'N/A':<12} {result['status']}")
    
    print("\n* TensorRT Single results are scaled to 100 samples")
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ
    if len(successful_results) > 1:
        print("\nğŸ“Š PERFORMANCE COMPARISON:")
        
        # åŸºæº–ã¨ãªã‚‹æœ€é€Ÿã®ãƒ¢ãƒ‡ãƒ«ã‚’è¦‹ã¤ã‘ã‚‹
        fastest_result = min(successful_results, 
                           key=lambda x: x.get('scaled_time_100', x.get('mean_time', float('inf'))))
        fastest_time = fastest_result.get('scaled_time_100', fastest_result.get('mean_time'))
        
        print(f"Baseline (fastest): {fastest_result['name']} - {fastest_time:.3f}s")
        print("-" * 50)
        
        for result in successful_results:
            current_time = result.get('scaled_time_100', result.get('mean_time'))
            speedup = current_time / fastest_time
            if speedup < 1.1:
                status = "ğŸŸ¢ Similar"
            elif speedup < 2.0:
                status = "ğŸŸ¡ Slower"
            else:
                status = "ğŸ”´ Much slower"
            
            print(f"{result['name']:<25} {speedup:.2f}x {status}")
    
    # GPUåŠ¹ç‡æ€§ã®è¡¨ç¤º
    print(f"\nğŸš€ GPU EFFICIENCY ANALYSIS:")
    batch_results = [r for r in successful_results if 'Batch' in r['name']]
    single_results = [r for r in successful_results if 'Single' in r['name']]
    
    if batch_results and single_results:
        batch_time = batch_results[0].get('mean_time', 0)
        single_time = single_results[0].get('scaled_time_100', 0)
        if single_time > 0:
            efficiency_gain = single_time / batch_time
            print(f"Batch processing efficiency: {efficiency_gain:.1f}x faster than single processing")
            print(f"GPU transfer reduction: ~{((100-32)/100)*100:.0f}% fewer transfers")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸƒâ€â™‚ï¸ Performance Benchmark: SavedModel vs ONNX vs TensorRT")
    print("="*60)
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    test_samples, test_labels = load_test_data(100)
    if test_samples is None:
        return
    
    print(f"Testing with {len(test_samples)} samples")
    print(f"Input shape: {test_samples.shape}")
    
    # å„ãƒ¢ãƒ‡ãƒ«ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
    results = []
    
    # SavedModel
    if os.path.exists('cifar10_vgg_model'):
        results.append(benchmark_savedmodel(test_samples))
    else:
        print("âš ï¸ SavedModel not found, skipping...")
        results.append({'name': 'SavedModel', 'status': 'âŒ Model file not found'})
    
    # ONNX
    if os.path.exists('model.onnx'):
        results.append(benchmark_onnx(test_samples))
    else:
        print("âš ï¸ ONNX model not found, skipping...")
        results.append({'name': 'ONNX', 'status': 'âŒ Model file not found'})
    
    # TensorRT ãƒãƒƒãƒå‡¦ç†
    if os.path.exists('model.trt'):
        results.append(benchmark_tensorrt_batch(test_samples, batch_size=32))
        results.append(benchmark_tensorrt_single(test_samples))
    else:
        print("âš ï¸ TensorRT engine not found, skipping...")
        results.append({'name': 'TensorRT (Batch 32)', 'status': 'âŒ Engine file not found'})
        results.append({'name': 'TensorRT (Single)', 'status': 'âŒ Engine file not found'})
    
    # çµæœè¡¨ç¤º
    print_benchmark_results(results)
    
    print(f"\nğŸ¯ Benchmark completed successfully!")
    print(f"ğŸ“ All results based on {len(test_samples)} CIFAR-10 test samples")

if __name__ == "__main__":
    main()