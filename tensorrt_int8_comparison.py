#!/usr/bin/env python3
"""
TensorRT INT8 Engine Generator with Proper Calibration
TensorRTå°‚ç”¨INT8ã‚¨ãƒ³ã‚¸ãƒ³ã®ç”Ÿæˆã¨æ€§èƒ½æ¯”è¼ƒ
"""
import time
import numpy as np
import statistics
import json
import os
from typing import Dict, List, Tuple, Any

def load_test_data():
    """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
    try:
        test_samples = np.load('test_samples.npy')
        test_labels = np.load('test_labels.npy')
        print(f"âœ… Loaded {len(test_samples)} test samples: {test_samples.shape}")
        return test_samples, test_labels
    except FileNotFoundError:
        print("âŒ Test data files not found!")
        return None, None

def calculate_accuracy(predictions: np.ndarray, true_labels: np.ndarray) -> float:
    """åˆ†é¡ç²¾åº¦ã®è¨ˆç®—"""
    predicted_classes = np.argmax(predictions, axis=1)
    true_classes = np.argmax(true_labels, axis=1) if true_labels.ndim > 1 else true_labels
    accuracy = np.mean(predicted_classes == true_classes)
    return accuracy

class EntropyCalibrator:
    """TensorRT INT8ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, calibration_data, batch_size=32, cache_file="calibration.cache"):
        import tensorrt as trt
        import pycuda.driver as cuda
        import pycuda.autoinit
        
        self.calibration_data = calibration_data.astype(np.float32)
        self.batch_size = batch_size
        self.cache_file = cache_file
        self.current_index = 0
        self.device_input = None
        self.cuda = cuda
        
        # GPU ãƒ¡ãƒ¢ãƒªç¢ºä¿
        if len(self.calibration_data) > 0:
            self.device_input = cuda.mem_alloc(self.calibration_data[0:self.batch_size].nbytes)
    
    def get_batch_size(self):
        return self.batch_size
    
    def get_batch(self, names):
        """ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿å–å¾—"""
        if self.current_index + self.batch_size > len(self.calibration_data):
            return None
        
        batch = self.calibration_data[self.current_index:self.current_index + self.batch_size]
        self.cuda.memcpy_htod(self.device_input, batch)
        self.current_index += self.batch_size
        
        return [int(self.device_input)]
    
    def read_calibration_cache(self):
        """ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿"""
        if os.path.exists(self.cache_file):
            with open(self.cache_file, "rb") as f:
                return f.read()
        return None
    
    def write_calibration_cache(self, cache):
        """ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›¸ãè¾¼ã¿"""
        with open(self.cache_file, "wb") as f:
            f.write(cache)

def build_tensorrt_int8_engine():
    """TensorRT INT8ã‚¨ãƒ³ã‚¸ãƒ³ã®ãƒ“ãƒ«ãƒ‰"""
    print("ğŸ”§ Building TensorRT INT8 engine with proper calibration...")
    
    try:
        import tensorrt as trt
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ï¼‰
        test_samples, _ = load_test_data()
        if test_samples is None:
            return False
        
        # TensorRT logger
        TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
        
        # ONNXãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
        onnx_model_path = 'model.onnx'
        if not os.path.exists(onnx_model_path):
            print("âŒ ONNX model not found!")
            return False
        
        builder = trt.Builder(TRT_LOGGER)
        
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä½œæˆ
        network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
        parser = trt.OnnxParser(network, TRT_LOGGER)
        
        with open(onnx_model_path, 'rb') as model:
            if not parser.parse(model.read()):
                print("âŒ Failed to parse ONNX model")
                for error in range(parser.num_errors):
                    print(f"   {parser.get_error(error)}")
                return False
        
        config = builder.create_builder_config()
        
        # INT8è¨­å®š
        if builder.platform_has_fast_int8:
            config.set_flag(trt.BuilderFlag.INT8)
            print("âœ… INT8 precision enabled")
            
            # ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®šï¼ˆã‚ˆã‚Šå¤šãã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼‰
            calibration_data = test_samples[:min(320, len(test_samples))]  # 10ãƒãƒƒãƒåˆ†
            calibrator = EntropyCalibrator(calibration_data, batch_size=32)
            
            # TensorRT 10.x ã§ã¯ IInt8EntropyCalibrator2 ã‚’ç¶™æ‰¿
            class TensorRTCalibrator(trt.IInt8EntropyCalibrator2):
                def __init__(self, calibrator):
                    trt.IInt8EntropyCalibrator2.__init__(self)
                    self.calibrator = calibrator
                
                def get_batch_size(self):
                    return self.calibrator.get_batch_size()
                
                def get_batch(self, names):
                    return self.calibrator.get_batch(names)
                
                def read_calibration_cache(self):
                    return self.calibrator.read_calibration_cache()
                
                def write_calibration_cache(self, cache):
                    return self.calibrator.write_calibration_cache(cache)
            
            config.int8_calibrator = TensorRTCalibrator(calibrator)
            print("âœ… INT8 calibrator configured")
            
        else:
            print("âš ï¸ INT8 not supported, using FP32")
            return False
        
        # æœ€é©åŒ–ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®š
        profile = builder.create_optimization_profile()
        profile.set_shape('input_1', [1, 32, 32, 3], [16, 32, 32, 3], [32, 32, 32, 3])
        config.add_optimization_profile(profile)
        
        # ã‚¨ãƒ³ã‚¸ãƒ³ãƒ“ãƒ«ãƒ‰
        print("   Building INT8 engine (this may take a while)...")
        serialized_engine = builder.build_serialized_network(network, config)
        
        if serialized_engine:
            with open('model_tensorrt_int8.trt', 'wb') as f:
                f.write(serialized_engine)
            print("âœ… TensorRT INT8 engine saved as model_tensorrt_int8.trt")
            return True
        else:
            print("âŒ Failed to build INT8 engine")
            return False
        
    except Exception as e:
        print(f"âŒ Error building INT8 engine: {e}")
        import traceback
        traceback.print_exc()
        return False

def benchmark_tensorrt_precision(engine_path: str, test_samples: np.ndarray, 
                                test_labels: np.ndarray, precision_name: str,
                                batch_size: int = 32, num_runs: int = 5) -> Dict[str, Any]:
    """TensorRTæ¨è«–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
    print(f"\nğŸ”„ Benchmarking TensorRT {precision_name}...")
    
    try:
        import tensorrt as trt
        import pycuda.driver as cuda
        import pycuda.autoinit
        
        # TensorRT logger
        TRT_LOGGER = trt.Logger(trt.Logger.WARNING)
        
        # ã‚¨ãƒ³ã‚¸ãƒ³èª­ã¿è¾¼ã¿æ™‚é–“æ¸¬å®š
        start_time = time.time()
        runtime = trt.Runtime(TRT_LOGGER)
        with open(engine_path, 'rb') as f:
            engine = runtime.deserialize_cuda_engine(f.read())
        context = engine.create_execution_context()
        load_time = time.time() - start_time
        
        # ãƒ†ãƒ³ã‚½ãƒ«æƒ…å ±å–å¾—
        input_names = []
        output_names = []
        for i in range(engine.num_io_tensors):
            name = engine.get_tensor_name(i)
            if engine.get_tensor_mode(name) == trt.TensorIOMode.INPUT:
                input_names.append(name)
            else:
                output_names.append(name)
        
        input_name = input_names[0]
        output_name = output_names[0]
        
        # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
        warmup_batch = test_samples[:min(batch_size, len(test_samples))]
        context.set_input_shape(input_name, [len(warmup_batch), 32, 32, 3])
        warmup_output_shape = context.get_tensor_shape(output_name)
        warmup_input_data = warmup_batch.astype(np.float32)
        warmup_output_data = np.empty(warmup_output_shape, dtype=np.float32)
        
        warmup_input_gpu = cuda.mem_alloc(warmup_input_data.nbytes)
        warmup_output_gpu = cuda.mem_alloc(warmup_output_data.nbytes)
        cuda.memcpy_htod(warmup_input_gpu, warmup_input_data)
        context.set_tensor_address(input_name, warmup_input_gpu)
        context.set_tensor_address(output_name, warmup_output_gpu)
        context.execute_async_v3(0)
        cuda.memcpy_dtoh(warmup_output_data, warmup_output_gpu)
        warmup_input_gpu.free()
        warmup_output_gpu.free()
        
        # ãƒãƒƒãƒå‡¦ç†ã§æ¸¬å®š
        num_batches = (len(test_samples) + batch_size - 1) // batch_size
        all_predictions = []
        inference_times = []
        
        for run in range(num_runs):
            run_predictions = []
            start_time = time.time()
            
            for batch_idx in range(num_batches):
                start_idx = batch_idx * batch_size
                end_idx = min(start_idx + batch_size, len(test_samples))
                current_batch_size = end_idx - start_idx
                
                batch_data = test_samples[start_idx:end_idx]
                
                # å…¥åŠ›å½¢çŠ¶è¨­å®š
                context.set_input_shape(input_name, [current_batch_size, 32, 32, 3])
                output_shape = context.get_tensor_shape(output_name)
                
                # ãƒ‡ãƒ¼ã‚¿æº–å‚™
                input_data = batch_data.astype(np.float32)
                output_data = np.empty(output_shape, dtype=np.float32)
                
                # GPU ãƒ¡ãƒ¢ãƒªç¢ºä¿
                input_gpu = cuda.mem_alloc(input_data.nbytes)
                output_gpu = cuda.mem_alloc(output_data.nbytes)
                
                # ãƒ‡ãƒ¼ã‚¿è»¢é€ãƒ»æ¨è«–ãƒ»çµæœå–å¾—
                cuda.memcpy_htod(input_gpu, input_data)
                context.set_tensor_address(input_name, input_gpu)
                context.set_tensor_address(output_name, output_gpu)
                context.execute_async_v3(0)
                cuda.memcpy_dtoh(output_data, output_gpu)
                
                # GPU ãƒ¡ãƒ¢ãƒªè§£æ”¾
                input_gpu.free()
                output_gpu.free()
                
                run_predictions.append(output_data)
            
            inference_time = time.time() - start_time
            inference_times.append(inference_time)
            
            if run == 0:  # æœ€åˆã®å®Ÿè¡Œçµæœã‚’ç²¾åº¦è¨ˆç®—ç”¨ã«ä¿å­˜
                all_predictions = np.vstack(run_predictions)
            
            print(f"  Run {run+1}: {inference_time:.4f}s ({num_batches} batches)")
        
        # ç²¾åº¦è¨ˆç®—
        accuracy = calculate_accuracy(all_predictions, test_labels)
        
        result = {
            'precision': precision_name,
            'framework': 'TensorRT',
            'load_time': load_time,
            'inference_times': inference_times,
            'mean_time': statistics.mean(inference_times),
            'std_time': statistics.stdev(inference_times),
            'samples_per_second': len(test_samples) / statistics.mean(inference_times),
            'accuracy': accuracy,
            'num_batches': num_batches,
            'batch_size': batch_size
        }
        
        print(f"âœ… {precision_name} - Load: {load_time:.3f}s, Inference: {result['mean_time']:.4f}s Â± {result['std_time']:.4f}s")
        print(f"   Throughput: {result['samples_per_second']:.1f} samples/sec, Accuracy: {accuracy:.4f}")
        
        return result
        
    except Exception as e:
        print(f"âŒ Error benchmarking {precision_name}: {e}")
        return None

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ TensorRT Complete Precision Comparison: FP32 vs FP16 vs INT8")
    print("=" * 70)
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    test_samples, test_labels = load_test_data()
    if test_samples is None:
        return
    
    # TensorRT INT8ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ãƒ“ãƒ«ãƒ‰
    if not build_tensorrt_int8_engine():
        print("âŒ Failed to build TensorRT INT8 engine")
        return
    
    print("\n" + "="*70)
    print("ğŸ”„ Running TensorRT Precision Benchmarks...")
    
    # æ—¢å­˜ã‚¨ãƒ³ã‚¸ãƒ³ã®ç¢ºèª
    engines = {
        'FP32': 'model.trt',
        'FP16': 'model_fp16.trt',
        'INT8': 'model_tensorrt_int8.trt'
    }
    
    results = {}
    
    for precision, engine_path in engines.items():
        if os.path.exists(engine_path):
            result = benchmark_tensorrt_precision(
                engine_path, test_samples, test_labels, precision
            )
            if result:
                results[precision] = result
        else:
            print(f"âš ï¸ {precision} engine not found: {engine_path}")
    
    # çµæœæ¯”è¼ƒ
    if len(results) >= 2:
        print("\nğŸ“Š TensorRT Precision Comparison Results:")
        print("=" * 90)
        print(f"{'Precision':<10} {'Time (s)':<10} {'Throughput':<15} {'Accuracy':<10} {'Size (MB)':<12} {'Speedup':<10}")
        print("-" * 90)
        
        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆFP32ï¼‰
        baseline_throughput = results.get('FP32', {}).get('samples_per_second', 1)
        
        for precision in ['FP32', 'FP16', 'INT8']:
            if precision in results:
                r = results[precision]
                engine_file = engines[precision]
                size_mb = os.path.getsize(engine_file) / (1024*1024) if os.path.exists(engine_file) else 0
                speedup = r['samples_per_second'] / baseline_throughput
                
                print(f"{precision:<10} {r['mean_time']:<10.4f} {r['samples_per_second']:<15.1f} {r['accuracy']:<10.4f} {size_mb:<12.1f} {speedup:<10.1f}x")
        
        print("\nğŸ¯ TensorRT Precision Analysis:")
        
        # é€Ÿåº¦æ¯”è¼ƒ
        if 'FP16' in results and 'FP32' in results:
            fp16_speedup = results['FP16']['samples_per_second'] / results['FP32']['samples_per_second']
            print(f"âš¡ FP16 speedup: {fp16_speedup:.1f}x over FP32")
        
        if 'INT8' in results and 'FP32' in results:
            int8_speedup = results['INT8']['samples_per_second'] / results['FP32']['samples_per_second']
            print(f"âš¡ INT8 speedup: {int8_speedup:.1f}x over FP32")
        
        # ç²¾åº¦æ¯”è¼ƒ
        accuracies = [r['accuracy'] for r in results.values()]
        accuracy_range = max(accuracies) - min(accuracies)
        print(f"ğŸ¯ Accuracy range: {min(accuracies):.4f} - {max(accuracies):.4f} (diff: {accuracy_range:.4f})")
        
        # ã‚µã‚¤ã‚ºæ¯”è¼ƒ
        sizes = {}
        for precision, engine_path in engines.items():
            if os.path.exists(engine_path):
                sizes[precision] = os.path.getsize(engine_path) / (1024*1024)
        
        if 'FP32' in sizes and 'INT8' in sizes:
            size_reduction = sizes['FP32'] / sizes['INT8']
            print(f"ğŸ’¾ INT8 size reduction: {size_reduction:.1f}x smaller than FP32")
        
        # çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        with open('tensorrt_precision_comparison.json', 'w') as f:
            json.dump(results, f, indent=2)
        print(f"\nğŸ’¾ Results saved to tensorrt_precision_comparison.json")

if __name__ == "__main__":
    main()