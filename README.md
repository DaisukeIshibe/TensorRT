# TensorRTæ¨è«–æœ€é©åŒ–ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ

## ğŸ“‹ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦

TensorFlowã§ä½œæˆã—ãŸCIFAR-10åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã‚’ã€ONNXã€TensorRTã«å¤‰æ›ã—ã€**ãƒãƒƒãƒå‡¦ç†æœ€é©åŒ–**ã¨**ç²¾åº¦åˆ¥æ€§èƒ½æ¯”è¼ƒ**ã‚’è¡Œã†åŒ…æ‹¬çš„ãªæ¤œè¨¼ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã€‚

### ğŸ¯ ä¸»è¦ç›®æ¨™
- **æ¨è«–ä¸€è²«æ€§ã®æ¤œè¨¼**: SavedModel â†’ ONNX â†’ TensorRT ã®å¤‰æ›ç²¾åº¦
- **ãƒãƒƒãƒå‡¦ç†æœ€é©åŒ–**: å˜ä¸€ã‚µãƒ³ãƒ—ãƒ«ã‹ã‚‰ãƒãƒƒãƒã‚µã‚¤ã‚º32ã¸ã®åŠ¹ç‡åŒ–
- **ç²¾åº¦åˆ¥æ€§èƒ½æ¯”è¼ƒ**: FP32, FP16, INT8 ã®å‡¦ç†é€Ÿåº¦ã¨ç²¾åº¦è©•ä¾¡
- **ã‚¯ãƒ­ã‚¹ãƒãƒ¼ã‚¸ãƒ§ãƒ³å¯¾å¿œ**: TensorRT 8.x ã¨ 10.x ã®äº’æ›æ€§æ¤œè¨¼

## ï¿½ ä¸»è¦ãªæˆæœ

### æ€§èƒ½æœ€é©åŒ–çµæœ
| ç²¾åº¦ | ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ (samples/sec) | é«˜é€ŸåŒ–å€ç‡ | ã‚¨ãƒ³ã‚¸ãƒ³ã‚µã‚¤ã‚º (MB) |
|------|---------------------------|-----------|-------------------|
| **TensorRT INT8** | **31,127.9** â­ | **14.8x** | **4.5** ğŸ’¾ |
| **TensorRT FP16** | **20,023.8** | **9.5x** | 8.5 |
| TensorRT FP32 | 2,108.2 | 1.0x | 16.4 |
| TF Lite INT8 | 1,160.1 | 3.6x | 3.7 |
| SavedModel | 555.6 | - | - |

### ãƒãƒƒãƒå‡¦ç†åŠ¹æœ
```
GPUè»¢é€æœ€é©åŒ–: 100å› â†’ 4å› (96%å‰Šæ¸›)
ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–: ãƒãƒƒãƒã‚µã‚¤ã‚º32ã§ã®ä¸¦åˆ—å‡¦ç†
å‡¦ç†é€Ÿåº¦å‘ä¸Š: 10.2å€é«˜é€ŸåŒ– (TensorRT batch vs single)
```

## ğŸ› ï¸ æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯

### é–‹ç™ºç’°å¢ƒ
- **TensorRT 10.x**: nvcr.io/nvidia/tensorrt:25.06-py3 (TensorRT 10.11.0)
- **TensorRT 8.x**: nvcr.io/nvidia/tensorrt:23.03-py3 (TensorRT 8.5.3)
- **TensorFlow**: nvcr.io/nvidia/tensorflow:25.02-tf2-py3 (TF 2.17.0)
- **CUDA**: 12.9 / GPUæ”¯æ´å¿…é ˆ

### ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°è¨€èª
- **Python**: æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã€æ€§èƒ½æ¯”è¼ƒã€ãƒ¢ãƒ‡ãƒ«å¤‰æ›
- **C++**: é«˜æ€§èƒ½æ¨è«–å®Ÿè£…ã€TensorRT APIç›´æ¥åˆ©ç”¨

## ğŸ“ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹æˆ

### ğŸ”§ ã‚¨ãƒ³ã‚¸ãƒ³ç”Ÿæˆãƒ»å¤‰æ›
```bash
cifar10.py                    # CIFAR-10ãƒ¢ãƒ‡ãƒ«è¨“ç·´
convert_to_onnx.py           # ONNXå¤‰æ›
convert_to_tensorrt.py       # TensorRTå¤‰æ› (æ±ç”¨)
convert_to_tensorrt_batch.py # ãƒãƒƒãƒæœ€é©åŒ–TensorRT
generate_trt8x_engine.py     # TensorRT 8.xç²¾åº¦åˆ¥ã‚¨ãƒ³ã‚¸ãƒ³ (FP32/FP16/INT8å¯¾å¿œ)
```

### ğŸš€ æ¨è«–ãƒ»æ¤œè¨¼ãƒ—ãƒ­ã‚°ãƒ©ãƒ 
```bash
tensorrt_inference_csv.cpp       # C++ TensorRT 10.x (ãƒãƒƒãƒå‡¦ç†)
tensorrt_inference_8x.cpp        # C++ TensorRT 8.x (ãƒ¬ã‚¬ã‚·ãƒ¼API)
precision_comparison_fp16.py     # FP32/FP16æ¯”è¼ƒ
tensorrt_int8_comparison.py      # å®Œå…¨ç²¾åº¦æ¯”è¼ƒ
complete_precision_comparison.py # æ¨ªæ–­ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯æ¯”è¼ƒ
```

### ğŸ“Š æ€§èƒ½æ¸¬å®šãƒ»æ¯”è¼ƒ
```bash
benchmark_performance.py     # åŒ…æ‹¬çš„æ€§èƒ½æ¸¬å®š
compare_models.py           # ãƒ¢ãƒ‡ãƒ«å½¢å¼é–“æ¯”è¼ƒ  
export_csv_data.py          # C++äº’æ›ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
```

### ğŸ³ å®Ÿè¡Œç’°å¢ƒ
```bash
docker_tf.sh     # TensorFlowç’°å¢ƒ
docker_trt.sh    # TensorRT 10.xç’°å¢ƒ  
docker_trt8x.sh  # TensorRT 8.xç’°å¢ƒ
```

## ğŸ¯ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### 1. ãƒ¢ãƒ‡ãƒ«è¨“ç·´ãƒ»ãƒ‡ãƒ¼ã‚¿æº–å‚™
```bash
# TensorFlowã‚³ãƒ³ãƒ†ãƒŠã§CIFAR-10ãƒ¢ãƒ‡ãƒ«è¨“ç·´
docker run --gpus all --rm -v $(pwd):/workspace -w /workspace \
  nvcr.io/nvidia/tensorflow:25.02-tf2-py3 python3 cifar10.py

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿CSVç”Ÿæˆ (C++äº’æ›)
docker run --gpus all --rm -v $(pwd):/workspace -w /workspace \
  nvcr.io/nvidia/tensorflow:25.02-tf2-py3 python3 export_csv_data.py
```

### 2. ONNXãƒ»TensorRTå¤‰æ›
```bash
# ONNXå¤‰æ›
docker run --gpus all --rm -v $(pwd):/workspace -w /workspace \
  nvcr.io/nvidia/tensorflow:25.02-tf2-py3 python3 convert_to_onnx.py

# TensorRT 10.x ãƒãƒƒãƒæœ€é©åŒ–ã‚¨ãƒ³ã‚¸ãƒ³ç”Ÿæˆ  
docker run --gpus all --rm -v $(pwd):/workspace -w /workspace \
  nvcr.io/nvidia/tensorrt:25.06-py3 python3 convert_to_tensorrt_batch.py

# TensorRT 8.x ç²¾åº¦åˆ¥ã‚¨ãƒ³ã‚¸ãƒ³ç”Ÿæˆ (FP32/FP16å¯¾å¿œ)
docker run --gpus all --rm -v $(pwd):/workspace -w /workspace \
  nvcr.io/nvidia/tensorrt:23.03-py3 python3 generate_trt8x_engine.py --all
```

### 3. æ€§èƒ½æ¯”è¼ƒå®Ÿè¡Œ
```bash
# å®Œå…¨ãªç²¾åº¦åˆ¥æ€§èƒ½æ¯”è¼ƒ
docker run --gpus all --rm -v $(pwd):/workspace -w /workspace \
  nvcr.io/nvidia/tensorrt:25.06-py3 python3 tensorrt_int8_comparison.py

# C++å®Ÿè£…æ¤œè¨¼
./docker_trt.sh  # TensorRT 10.x
./docker_trt8x.sh # TensorRT 8.x
```

## ğŸ“Š ä¸»è¦ãªæ¤œè¨¼çµæœ

### 1. ç²¾åº¦åˆ¥æ€§èƒ½æ¯”è¼ƒ
**TensorRTå°‚ç”¨æœ€é©åŒ–**ã«ã‚ˆã‚Šã€**INT8ã§14.8å€ã®é«˜é€ŸåŒ–**ã‚’å®Ÿç¾ï¼š
- ç²¾åº¦åŠ£åŒ–ãªã—ï¼ˆå…¨ç²¾åº¦ã§åŒä¸€ã®åˆ†é¡ç²¾åº¦ï¼‰
- ã‚¨ãƒ³ã‚¸ãƒ³ã‚µã‚¤ã‚º73%å‰Šæ¸›
- GPUæœ€é©åŒ–ã«ã‚ˆã‚‹åœ§å€’çš„ãªæ€§èƒ½å‘ä¸Š

### 2. ãƒãƒƒãƒå‡¦ç†æœ€é©åŒ–
**96%ã®GPUè»¢é€å‰Šæ¸›**ã§ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’å¤§å¹…æ”¹å–„ï¼š
- å˜ä¸€å‡¦ç†: 100å›ã®GPUè»¢é€
- ãƒãƒƒãƒå‡¦ç†: 4å›ã®GPUè»¢é€ (ãƒãƒƒãƒã‚µã‚¤ã‚º32)
- 10.2å€ã®å‡¦ç†é€Ÿåº¦å‘ä¸Š

### 3. ã‚¯ãƒ­ã‚¹ãƒãƒ¼ã‚¸ãƒ§ãƒ³äº’æ›æ€§
**TensorRT 8.x ã¨ 10.x ã®æ€§èƒ½ã‚’æ¯”è¼ƒæ¤œè¨¼**ï¼š
- æ€§èƒ½å·®: 1.2% (10.xæœ‰åˆ©)
- ã‚¨ãƒ³ã‚¸ãƒ³ã‚µã‚¤ã‚º: 71%å‰Šæ¸› (10.xæœ‰åˆ©)
- API: 10.xã§ã‚ˆã‚Šç›´æ„Ÿçš„ãªã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹

## ğŸ¯ æŠ€è¡“çš„ãƒã‚¤ãƒ©ã‚¤ãƒˆ

### ãƒãƒƒãƒå‡¦ç†æœ€é©åŒ–
```cpp
// å‹•çš„ãƒãƒƒãƒã‚µã‚¤ã‚ºå¯¾å¿œ (TensorRT 10.x)
context.set_input_shape(input_name, [current_batch_size, 32, 32, 3]);
context.set_tensor_address(input_name, input_gpu);
context.execute_async_v3(0);
```

### æœ€é©åŒ–ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®š
```python
# ãƒãƒƒãƒæœ€é©åŒ–ã‚¨ãƒ³ã‚¸ãƒ³ç”Ÿæˆ
profile.set_shape('input_1', 
                 [1, 32, 32, 3],    # min
                 [16, 32, 32, 3],   # opt  
                 [32, 32, 32, 3])   # max
```

### INT8é‡å­åŒ–
```python
# EntropyCalibratorä½¿ç”¨
config.int8_calibrator = TensorRTCalibrator(calibrator)
config.set_flag(trt.BuilderFlag.INT8)
```

## ğŸ† ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ

**æœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§å®Ÿç¾ã—ãŸæœ€é«˜æ€§èƒ½**:
- **æœ€é«˜é€Ÿåº¦**: TensorRT INT8 (31,127.9 samples/sec)
- **æœ€å°ã‚µã‚¤ã‚º**: TensorRT INT8 (4.5 MB)  
- **æœ€é«˜åŠ¹ç‡**: ãƒãƒƒãƒå‡¦ç†ã«ã‚ˆã‚‹96%GPUè»¢é€å‰Šæ¸›
- **äº’æ›æ€§**: TensorRT 8.x/10.x ã‚¯ãƒ­ã‚¹ãƒãƒ¼ã‚¸ãƒ§ãƒ³å¯¾å¿œ

**å®Ÿç”¨æ€§**: æœ¬ç•ªç’°å¢ƒã§ã®é«˜é€Ÿæ¨è«–ã€ã‚¨ãƒƒã‚¸ãƒ‡ãƒã‚¤ã‚¹ã§ã®è»½é‡åŒ–ã€é–‹ç™ºç’°å¢ƒã§ã®æ¤œè¨¼ã«å¯¾å¿œã—ãŸåŒ…æ‹¬çš„ãªã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã€‚
## ğŸ”§ æŠ€è¡“çš„èª²é¡Œã¨è§£æ±ºç­–

### 1. ãƒãƒƒãƒå‡¦ç†æœ€é©åŒ–
**èª²é¡Œ**: å˜ä¸€ã‚µãƒ³ãƒ—ãƒ«å‡¦ç†ã«ã‚ˆã‚‹éåŠ¹ç‡ãªGPUåˆ©ç”¨
**è§£æ±ºç­–**: 
```cpp
// ãƒãƒƒãƒã‚µã‚¤ã‚º32ã§ã®å‹•çš„å‡¦ç†å®Ÿè£…
const int batch_size = 32;
vector<float> input_batch(current_batch_size * input_size_per_sample);

// TensorRTå‹•çš„ãƒãƒƒãƒã‚µã‚¤ã‚ºè¨­å®š
Dims4 inputShape{current_batch_size, 32, 32, 3};
context->setInputShape(input_name.c_str(), inputShape);
```

### 2. TensorRTæœ€é©åŒ–ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
**èª²é¡Œ**: `Error Code 4: Network has dynamic inputs, but no optimization profile`
**è§£æ±ºç­–**:
```python
profile = builder.create_optimization_profile()
profile.set_shape(input_name, 
                 min=(1, 32, 32, 3),     # æœ€å°ãƒãƒƒãƒã‚µã‚¤ã‚º
                 opt=(16, 32, 32, 3),    # æœ€é©ãƒãƒƒãƒã‚µã‚¤ã‚º  
                 max=(32, 32, 32, 3))    # æœ€å¤§ãƒãƒƒãƒã‚µã‚¤ã‚º
config.add_optimization_profile(profile)
```

### 3. TensorRT 10.x APIç§»è¡Œ
**èª²é¡Œ**: ãƒ¬ã‚¬ã‚·ãƒ¼API (`execute()`) ã‹ã‚‰ç¾ä»£çš„API ã¸ã®ç§»è¡Œ
**è§£æ±ºç­–**:
```cpp
// TensorRT 10.x API
context->set_tensor_address(input_name, input_gpu);
context->set_tensor_address(output_name, output_gpu);
context->execute_async_v3(0);
```

### 4. ãƒ¡ãƒ¢ãƒªå®‰å…¨æ€§
**èª²é¡Œ**: æ‰‹å‹•ãƒ¡ãƒ¢ãƒªç®¡ç†ã«ã‚ˆã‚‹æ½œåœ¨çš„ãƒªã‚¹ã‚¯
**è§£æ±ºç­–**:
```cpp
// smart pointerä½¿ç”¨
std::unique_ptr<nvinfer1::ICudaEngine> engine;
std::unique_ptr<nvinfer1::IExecutionContext> context;
```

## ğŸ’¡ ä½¿ç”¨ä¸Šã®æ³¨æ„ç‚¹

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã®ãŸã‚ã®ãƒ’ãƒ³ãƒˆ
1. **ãƒãƒƒãƒã‚µã‚¤ã‚º**: 32ãŒæœ€é©ï¼ˆGPU ãƒ¡ãƒ¢ãƒªã¨ã®ãƒãƒ©ãƒ³ã‚¹ï¼‰
2. **ãƒ‡ãƒ¼ã‚¿å‹**: INT8ã§æœ€é«˜é€Ÿåº¦ã€FP16ã§ãƒãƒ©ãƒ³ã‚¹
3. **ãƒ¡ãƒ¢ãƒªç®¡ç†**: ãƒãƒƒãƒå˜ä½ã§ã®GPUè»¢é€ã‚’æ´»ç”¨
4. **ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®š**: ä½¿ç”¨ã‚±ãƒ¼ã‚¹ã«å¿œã˜ãŸå‹•çš„å½¢çŠ¶è¨­å®š

### ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
- **GPU ãƒ¡ãƒ¢ãƒªä¸è¶³**: ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’16ä»¥ä¸‹ã«èª¿æ•´
- **ç²¾åº¦å•é¡Œ**: ã‚¨ãƒ³ã‚¸ãƒ³å†ç”Ÿæˆã€ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ç¢ºèª
- **API ã‚¨ãƒ©ãƒ¼**: TensorRTãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¨APIã®å¯¾å¿œç¢ºèª
- **ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã‚¨ãƒ©ãƒ¼**: Dockerã‚³ãƒ³ãƒ†ãƒŠå†…ã§ã®ãƒ“ãƒ«ãƒ‰æ¨å¥¨

## ğŸš§ ä»Šå¾Œã®æ‹¡å¼µäºˆå®š

- [ ] **å‹•çš„å½¢çŠ¶å¯¾å¿œ**: å¯å¤‰å…¥åŠ›ã‚µã‚¤ã‚ºã¸ã®å¯¾å¿œ
- [ ] **ãƒãƒ«ãƒGPUå¯¾å¿œ**: åˆ†æ•£æ¨è«–ã‚·ã‚¹ãƒ†ãƒ 
- [ ] **ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æ¨è«–**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
- [ ] **Triton Integration**: NVIDIA Tritonã‚µãƒ¼ãƒãƒ¼é€£æº
- [ ] **ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è‡ªå‹•åŒ–**: CI/CDãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆ

## ğŸ“ ã‚µãƒãƒ¼ãƒˆãƒ»ã‚³ãƒ³ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³

### å ±å‘Šãƒ»è³ªå•
- Issueä½œæˆæ™‚ã¯å®Ÿè¡Œç’°å¢ƒï¼ˆGPUã€TensorRTãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼‰ã‚’æ˜è¨˜
- å†ç¾å¯èƒ½ãªæœ€å°ã‚³ãƒ¼ãƒ‰ä¾‹ã‚’æä¾›
- ãƒ­ã‚°ãƒ»ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®è©³ç´°ã‚’æ·»ä»˜

### é–‹ç™ºã¸ã®å‚åŠ 
- ãƒ•ã‚©ãƒ¼ã‚¯ â†’ ãƒ–ãƒ©ãƒ³ãƒä½œæˆ â†’ ãƒ—ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
- ã‚³ãƒ¼ãƒ‰ã‚¹ã‚¿ã‚¤ãƒ«: C++17ã€Python PEP8æº–æ‹ 
- ãƒ†ã‚¹ãƒˆ: å„ç’°å¢ƒã§ã®å‹•ä½œç¢ºèªå¿…é ˆ

---

**ğŸ¯ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç›®æ¨™é”æˆ**: SavedModel â†’ ONNX â†’ TensorRT ã®æ¨è«–ä¸€è²«æ€§æ¤œè¨¼ã€ãƒãƒƒãƒå‡¦ç†æœ€é©åŒ–ã€ç²¾åº¦åˆ¥æ€§èƒ½æ¯”è¼ƒã€ã‚¯ãƒ­ã‚¹ãƒãƒ¼ã‚¸ãƒ§ãƒ³å¯¾å¿œã‚’ã™ã¹ã¦å®Œäº†ã—ã¾ã—ãŸã€‚

**è§£æ±ºç­–:**
```python
# convert_to_tensorrt_batch.py ã§ã®å¯¾å¿œ
profile = builder.create_optimization_profile()
profile.set_shape(input_name, 
                 min=(1, 32, 32, 3),     # æœ€å°ãƒãƒƒãƒã‚µã‚¤ã‚º
                 opt=(16, 32, 32, 3),    # æœ€é©ãƒãƒƒãƒã‚µã‚¤ã‚º  
                 max=(32, 32, 32, 3))    # æœ€å¤§ãƒãƒƒãƒã‚µã‚¤ã‚º
config.add_optimization_profile(profile)
```

### 3. TensorRT APIäº’æ›æ€§ (8.x â†’ 10.x)
**å•é¡Œ:** æ—¢å­˜ã‚³ãƒ¼ãƒ‰ãŒTensorRT 8.x APIã‚’ä½¿ç”¨ã—ã¦ã„ãŸãŒã€ã‚³ãƒ³ãƒ†ãƒŠã¯TensorRT 10.11.0

**è§£æ±ºç­–:**
- `get_binding_index()` â†’ tensor-based API
- `enqueueV2()` â†’ `enqueueV3()`
- `max_workspace_size` â†’ `set_memory_pool_limit()`
- optimization profileã®è¿½åŠ ãŒå¿…é ˆ

### 4. C++ç‰ˆTensorRT 10.x APIç§»è¡Œ
**å•é¡Œ:** `Parameter check failed, condition: inputDimensionSpecified && inputShapesSpecified`

**è§£æ±ºç­–:**
```cpp
// TensorRT 10.xå¿…é ˆã®ã‚·ã‚§ã‚¤ãƒ—è¨­å®š
Dims4 inputShape{current_batch_size, 32, 32, 3}; // NHWCå½¢å¼
context->setInputShape(input_name.c_str(), inputShape);

// æ–°ã—ã„ãƒ†ãƒ³ã‚½ãƒ«ãƒ™ãƒ¼ã‚¹APIä½¿ç”¨
context->setTensorAddress(input_name.c_str(), d_input);
context->setTensorAddress(output_name.c_str(), d_output);
context->enqueueV3(0);

// smart pointerä½¿ç”¨ã§ãƒ¡ãƒ¢ãƒªå®‰å…¨æ€§ç¢ºä¿
auto runtime = std::shared_ptr<IRuntime>(createInferRuntime(logger));
auto engine = std::shared_ptr<ICudaEngine>(runtime->deserializeCudaEngine(engineData.data(), size));
auto context = std::shared_ptr<IExecutionContext>(engine->createExecutionContext());
```

### 5. CSVå½¢å¼ã§ã®ãƒ‡ãƒ¼ã‚¿äº’æ›æ€§ç¢ºä¿
**èª²é¡Œ:** Pythonç‰ˆã¨C++ç‰ˆã§ã®åŒä¸€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä½¿ç”¨

**è§£æ±ºç­–:**
```cpp
// C++ç‰ˆã§ã®CSVèª­ã¿è¾¼ã¿å®Ÿè£…
vector<vector<float>> loadTestSamplesFromCSV() {
    ifstream file("test_samples.csv");
    // ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ãƒ”ã‚¯ã‚»ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    // sample_id, pixel_0, pixel_1, ..., pixel_3071 format
}
```

## ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ - ãƒãƒƒãƒå‡¦ç†å¯¾å¿œç‰ˆ

### æˆåŠŸæ™‚ã®æœ€çµ‚ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ
```
TensorRT/
â”œâ”€â”€ cifar10.py                          # CIFAR-10ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”œâ”€â”€ convert_to_onnx.py                  # ONNXå¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆ  
â”œâ”€â”€ convert_to_tensorrt.py              # TensorRTå¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆ (æ—§ç‰ˆ)
â”œâ”€â”€ convert_to_tensorrt_batch.py        # ğŸ†• ãƒãƒƒãƒæœ€é©åŒ–TensorRTå¤‰æ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”œâ”€â”€ compare_models.py                   # å…¨ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒã‚¹ã‚¯ãƒªãƒ—ãƒˆ (TensorRT 10.xå¯¾å¿œ)
â”œâ”€â”€ export_csv_data.py                  # CSVå½¢å¼ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿å‡ºåŠ›ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”œâ”€â”€ tensorrt_inference.cpp              # C++ TensorRTæ¨è«–ãƒ—ãƒ­ã‚°ãƒ©ãƒ  (æ—§ç‰ˆ)
â”œâ”€â”€ tensorrt_inference_final.cpp        # C++ TensorRTæ¨è«–ãƒ—ãƒ­ã‚°ãƒ©ãƒ  (TensorRT 10.xå¯¾å¿œ)
â”œâ”€â”€ tensorrt_inference_csv.cpp          # ğŸ†• C++ TensorRTæ¨è«–ãƒ—ãƒ­ã‚°ãƒ©ãƒ  (ãƒãƒƒãƒå¯¾å¿œãƒ»CSVäº’æ›ç‰ˆ)
â”œâ”€â”€ cifar10_vgg_model/                  # TensorFlow SavedModel
â”œâ”€â”€ model.onnx                          # ONNX ãƒ¢ãƒ‡ãƒ« (15.36MB)
â”œâ”€â”€ model.trt                           # ğŸ†• ãƒãƒƒãƒæœ€é©åŒ–TensorRTã‚¨ãƒ³ã‚¸ãƒ³ (17.2MB)
â”œâ”€â”€ test_samples.npy                    # ãƒ†ã‚¹ãƒˆç”»åƒ (100, 32, 32, 3)
â”œâ”€â”€ test_labels.npy                     # ãƒ†ã‚¹ãƒˆãƒ©ãƒ™ãƒ« (100, 1)
â”œâ”€â”€ test_samples.csv                    # ğŸ†• ãƒ†ã‚¹ãƒˆç”»åƒ (CSVå½¢å¼ã€5.8MB)
â”œâ”€â”€ test_labels.csv                     # ğŸ†• ãƒ†ã‚¹ãƒˆãƒ©ãƒ™ãƒ« (CSVå½¢å¼)
â”œâ”€â”€ verification_samples.csv            # ğŸ†• æ¤œè¨¼ç”¨ã‚µãƒ³ãƒ—ãƒ« (CSVå½¢å¼)
â”œâ”€â”€ savedmodel_predictions_final.npy
â”œâ”€â”€ onnx_predictions_final.npy
â””â”€â”€ tensorrt_predictions_final.npy
```

### ğŸ†• æ–°è¦è¿½åŠ ãƒ•ã‚¡ã‚¤ãƒ« (ãƒãƒƒãƒå‡¦ç†å¯¾å¿œ)

#### convert_to_tensorrt_batch.py
- **ç›®çš„**: ãƒãƒƒãƒã‚µã‚¤ã‚º32å¯¾å¿œã®æœ€é©åŒ–ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®š
- **ç‰¹å¾´**: min=1, opt=16, max=32 ã®ãƒãƒƒãƒã‚µã‚¤ã‚ºå¯¾å¿œ
- **å‡ºåŠ›**: ãƒãƒƒãƒæœ€é©åŒ–ã•ã‚ŒãŸ`model.trt`

#### tensorrt_inference_csv.cpp  
- **ç›®çš„**: ãƒãƒƒãƒã‚µã‚¤ã‚º32ã§ã®C++æ¨è«–å®Ÿè£…
- **ç‰¹å¾´**: 
  - CSVå½¢å¼ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å¯¾å¿œ
  - å‹•çš„ãƒãƒƒãƒã‚µã‚¤ã‚ºå‡¦ç† (`setInputShape()`)
  - TensorRT 10.x APIå®Œå…¨å¯¾å¿œ (`shared_ptr`ä½¿ç”¨)
  - ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ– (ãƒãƒƒãƒå˜ä½GPUè»¢é€)

#### test_samples.csv / test_labels.csv
- **ç›®çš„**: Python-C++é–“ã§ã®ãƒ‡ãƒ¼ã‚¿äº’æ›æ€§ç¢ºä¿
- **å½¢å¼**: `sample_id,pixel_0,pixel_1,...,pixel_3071`
- **ã‚µã‚¤ã‚º**: 5.8MB (100ã‚µãƒ³ãƒ—ãƒ« Ã— 3072ãƒ”ã‚¯ã‚»ãƒ«)
â”œâ”€â”€ test_samples.csv               # ãƒ†ã‚¹ãƒˆç”»åƒ (CSVå½¢å¼ã€616KB)
â”œâ”€â”€ test_labels.csv                # ãƒ†ã‚¹ãƒˆãƒ©ãƒ™ãƒ« (CSVå½¢å¼)
â”œâ”€â”€ verification_samples.csv       # æ¤œè¨¼ç”¨ã‚µãƒ³ãƒ—ãƒ« (CSVå½¢å¼)
â”œâ”€â”€ savedmodel_predictions_final.npy
â”œâ”€â”€ onnx_predictions_final.npy
â””â”€â”€ tensorrt_predictions_final.npy
```

### é‡è¦ãªã‚¹ã‚¯ãƒªãƒ—ãƒˆ

#### cifar10.py
- VGGã‚¹ã‚¿ã‚¤ãƒ«CNN (74.55%ç²¾åº¦)
- SavedModelå‡ºåŠ›
- ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆ

#### compare_models.py (TensorRT 10.xå¯¾å¿œç‰ˆ)
- Keras 3.xäº’æ›SavedModelèª­ã¿è¾¼ã¿
- ONNX Runtimeæ¨è«–
- TensorRT 10.x APIä½¿ç”¨
- optimization profileè‡ªå‹•è¨­å®š
- è©³ç´°ãªç²¾åº¦ãƒ»ä¸€è²«æ€§ãƒ¬ãƒãƒ¼ãƒˆ

## å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰é›† - ãƒãƒƒãƒå‡¦ç†å¯¾å¿œç‰ˆ

### ğŸš€ å®Œå…¨ãƒãƒƒãƒå‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
```bash
# 1. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ (100ã‚µãƒ³ãƒ—ãƒ«)
docker run --rm --gpus all -v $(pwd):/workspace -w /workspace \
  nvcr.io/nvidia/tensorflow:25.02-tf2-py3 python3 -c "
import tensorflow as tf; import numpy as np
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0
test_samples, test_labels = x_test[:100], y_test[:100]
np.save('test_samples.npy', test_samples); np.save('test_labels.npy', test_labels)
print(f'âœ… Saved {len(test_samples)} samples: {test_samples.shape}')
"

# 2. CSVå½¢å¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
docker run --rm --gpus all -v $(pwd):/workspace -w /workspace \
  nvcr.io/nvidia/tensorflow:25.02-tf2-py3 python3 export_csv_data.py

# 3. ãƒãƒƒãƒæœ€é©åŒ–TensorRTã‚¨ãƒ³ã‚¸ãƒ³ç”Ÿæˆ
docker run --rm --gpus all -v $(pwd):/workspace -w /workspace \
  nvcr.io/nvidia/tensorrt:25.06-py3 python3 convert_to_tensorrt_batch.py

# 4. Pythonç‰ˆãƒãƒƒãƒæ¨è«–ãƒ†ã‚¹ãƒˆ
docker run --rm --gpus all -v $(pwd):/workspace -w /workspace \
  nvcr.io/nvidia/tensorrt:25.06-py3 python3 -c "
# (Pythonç‰ˆãƒãƒƒãƒæ¨è«–æ¤œè¨¼ã‚³ãƒ¼ãƒ‰ - 32ã‚µãƒ³ãƒ—ãƒ«Ã—3ãƒãƒƒãƒå‡¦ç†)
import tensorrt as trt; import numpy as np
# ... (ãƒãƒƒãƒå‡¦ç†ã‚³ãƒ¼ãƒ‰) ...
"

# 5. C++ç‰ˆãƒãƒƒãƒæ¨è«–ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ãƒ»å®Ÿè¡Œ
docker run --rm --gpus all -v $(pwd):/workspace -w /workspace \
  nvcr.io/nvidia/tensorrt:25.06-py3 bash -c "
g++ -std=c++17 \
    -I/usr/local/cuda-12.9/targets/x86_64-linux/include \
    -I/usr/include/x86_64-linux-gnu \
    -L/usr/local/cuda-12.9/targets/x86_64-linux/lib \
    -L/usr/lib/x86_64-linux-gnu \
    -lnvinfer -lnvonnxparser -lcudart \
    tensorrt_inference_csv.cpp -o tensorrt_inference_csv && \
./tensorrt_inference_csv
"
```

### ãƒãƒƒãƒå‡¦ç†ç¢ºèªç”¨ã‚³ãƒãƒ³ãƒ‰
```bash
# ãƒãƒƒãƒå‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³ã®æƒ…å ±ç¢ºèª
docker run --rm --gpus all -v $(pwd):/workspace -w /workspace \
  nvcr.io/nvidia/tensorrt:25.06-py3 python3 -c "
import tensorrt as trt
runtime = trt.Runtime(trt.Logger(trt.Logger.WARNING))
with open('model.trt', 'rb') as f:
    engine = runtime.deserialize_cuda_engine(f.read())
print(f'Engine loaded: {engine is not None}')
print(f'Max batch size supported: 32')
print(f'Optimization profile: min=1, opt=16, max=32')
"

# CSV ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºç¢ºèª
ls -lh test_*.csv test_*.npy | awk '{print $5, $9}'
```

### ğŸ”§ å€‹åˆ¥ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆå®Ÿè¡Œ

#### C++ãƒãƒƒãƒå‡¦ç†ã‚³ãƒ³ãƒ‘ã‚¤ãƒ« (TensorRTã‚³ãƒ³ãƒ†ãƒŠå†…)
```bash
# TensorRT 10.x + ãƒãƒƒãƒå¯¾å¿œç‰ˆã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
g++ -std=c++17 \
    -I/usr/local/cuda-12.9/targets/x86_64-linux/include \
    -I/usr/include/x86_64-linux-gnu \
    -L/usr/local/cuda-12.9/targets/x86_64-linux/lib \
    -L/usr/lib/x86_64-linux-gnu \
    -lnvinfer -lnvonnxparser -lcudart \
    tensorrt_inference_csv.cpp -o tensorrt_inference_csv

# ãƒãƒƒãƒæ¨è«–å®Ÿè¡Œ (32ã‚µãƒ³ãƒ—ãƒ«Ã—3ãƒãƒƒãƒ)
./tensorrt_inference_csv
```

#### ãƒãƒƒãƒå‡¦ç†æ€§èƒ½æ¸¬å®š
```bash
# GPUä½¿ç”¨ç‡ã¨ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ç›£è¦–
nvidia-smi -l 1 &

# ãƒãƒƒãƒå‡¦ç†æ¨è«–ã®å®Ÿè¡Œæ™‚é–“æ¸¬å®š
time docker run --rm --gpus all -v $(pwd):/workspace -w /workspace \
  nvcr.io/nvidia/tensorrt:25.06-py3 ./tensorrt_inference_csv
```
## ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ± - ãƒãƒƒãƒå‡¦ç†å¯¾å¿œ

### ğŸš€ ãƒãƒƒãƒå‡¦ç†æ€§èƒ½æ”¹å–„
| æŒ‡æ¨™ | ä¿®æ­£å‰ (å˜ä¸€) | ä¿®æ­£å¾Œ (ãƒãƒƒãƒ32) | æ”¹å–„ç‡ |
|-----|-------------|----------------|--------|
| **GPUè»¢é€å›æ•°** | 100å› | 4å› | **96%å‰Šæ¸›** |
| **ãƒ¡ãƒ¢ãƒªåŠ¹ç‡** | ä½ | é«˜ | **å¤§å¹…æ”¹å–„** |
| **GPUåˆ©ç”¨ç‡** | æ–­ç¶šçš„ | é€£ç¶šçš„ | **åŠ¹ç‡å‘ä¸Š** |
| **æ¨è«–ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ** | åŸºæº–å€¤ | å‘ä¸Š | **ãƒãƒƒãƒåŠ¹æœ** |

### æ¨è«–æ™‚é–“ (å‚è€ƒå€¤)
- **SavedModel**: TensorFlowæœ€é©åŒ–æ¸ˆã¿
- **ONNX**: ONNX Runtime GPU  
- **TensorRT (å˜ä¸€)**: é«˜æ€§èƒ½ (1ã‚µãƒ³ãƒ—ãƒ«ãšã¤)
- **TensorRT (ãƒãƒƒãƒ32)**: **æœ€é«˜æ€§èƒ½** (ãƒãƒƒãƒæœ€é©åŒ–ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«é©ç”¨)

### ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ - ãƒãƒƒãƒå‡¦ç†å¯¾å¿œ
```
GPU Memory: 1671 MB (RTX 3050 Ti)
TensorRT Workspace: 1GBè¨­å®š
ãƒãƒƒãƒã‚µã‚¤ã‚º: 32ã‚µãƒ³ãƒ—ãƒ«
å…¥åŠ›ãƒ¡ãƒ¢ãƒª: 32 Ã— 32 Ã— 32 Ã— 3 Ã— 4 bytes = 393KB/batch  
å‡ºåŠ›ãƒ¡ãƒ¢ãƒª: 32 Ã— 10 Ã— 4 bytes = 1.28KB/batch
```

### ãƒãƒƒãƒã‚µã‚¤ã‚ºåˆ¥æœ€é©åŒ–ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
```
æœ€å°ãƒãƒƒãƒã‚µã‚¤ã‚º: 1  (ãƒ‡ãƒãƒƒã‚°ãƒ»ãƒ†ã‚¹ãƒˆç”¨)
æœ€é©ãƒãƒƒãƒã‚µã‚¤ã‚º: 16 (ãƒãƒ©ãƒ³ã‚¹é‡è¦–)
æœ€å¤§ãƒãƒƒãƒã‚µã‚¤ã‚º: 32 (æœ€é«˜ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ)
```

## æ¤œè¨¼ã®ä¾¡å€¤ - ãƒãƒƒãƒå‡¦ç†å¯¾å¿œå®Œäº†

### ğŸ¯ æŠ€è¡“çš„é”æˆ
1. **APIç§»è¡Œã®å®Œå…¨å¯¾å¿œ**: TensorRT 8.x â†’ 10.x (Pythonã¨C++ä¸¡å¯¾å¿œ)
2. **ãƒãƒƒãƒå‡¦ç†ã®å®Ÿè£…**: å˜ä¸€ã‚µãƒ³ãƒ—ãƒ« â†’ ãƒãƒƒãƒã‚µã‚¤ã‚º32ã¸ã®æœ€é©åŒ–
3. **å‹•çš„ãƒãƒƒãƒã‚µã‚¤ã‚ºå¯¾å¿œ**: æœ€é©åŒ–ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®š (min=1, opt=16, max=32)
4. **ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯äº’æ›æ€§**: Keras 2.x â†’ 3.xå¯¾å¿œ  
5. **å‹å¤‰æ›ãƒã‚§ãƒ¼ãƒ³**: TF â†’ ONNX â†’ TRT (ãƒãƒƒãƒå¯¾å¿œ)
6. **æ•°å€¤ç²¾åº¦æ¤œè¨¼**: å®Ÿç”¨ãƒ¬ãƒ™ãƒ«ã§ã®ä¸€è²«æ€§ç¢ºèª
7. **C++æ¨è«–ã®å®Œå…¨å®Ÿè£…**: TensorRT 10.x APIã§ã®å‹•çš„ãƒãƒƒãƒå¯¾å¿œ
8. **ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–**: GPUè»¢é€å›æ•°ã‚’96%å‰Šæ¸› (100å›â†’4å›)

### ğŸ’¡ å®Ÿç”¨çš„ä¾¡å€¤
1. **æœ¬ç•ªç’°å¢ƒã§ã®ä¿¡é ¼æ€§**: ãƒ¢ãƒ‡ãƒ«å¤‰æ›å¾Œã®æ€§èƒ½ä¿è¨¼
2. **ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆå®‰å…¨æ€§**: æ¨è«–çµæœã®äºˆæ¸¬å¯èƒ½æ€§
3. **æœ€é©åŒ–åŠ¹æœæ¸¬å®š**: TensorRTãƒãƒƒãƒå‡¦ç†ã«ã‚ˆã‚‹æ€§èƒ½å‘ä¸Šã®ç¢ºèª
4. **äº’æ›æ€§ä¿è¨¼**: è¤‡æ•°ç’°å¢ƒã§ã®å‹•ä½œç¢ºèª
5. **å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³**: Python/C++ä¸¡ç’°å¢ƒã§ã®å‹•ä½œæ¤œè¨¼
6. **ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£**: ãƒãƒƒãƒã‚µã‚¤ã‚º32ã§ã®é«˜ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ¨è«–
7. **GPUåŠ¹ç‡æœ€å¤§åŒ–**: é€£ç¶šçš„ãªGPUåˆ©ç”¨ã«ã‚ˆã‚‹é›»åŠ›åŠ¹ç‡å‘ä¸Š

## Pythonç‰ˆã¨C++ç‰ˆã®ä¸€è‡´æ€§æ¤œè¨¼çµŒé - ãƒãƒƒãƒå‡¦ç†å¯¾å¿œç‰ˆ

### ğŸ¯ æ¤œè¨¼ç›®çš„
TensorRTæ¨è«–ã«ãŠã„ã¦ã€Pythonç‰ˆã¨C++ç‰ˆã§åŒä¸€ã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦**ãƒãƒƒãƒã‚µã‚¤ã‚º32ã§ã®å‡¦ç†**ã§ã‚‚å®Œå…¨ã«ä¸€è‡´ã—ãŸäºˆæ¸¬çµæœãŒå¾—ã‚‰ã‚Œã‚‹ã‹ã‚’æ¤œè¨¼ã€‚

### ğŸ”„ æ¤œè¨¼ãƒ—ãƒ­ã‚»ã‚¹

#### ç¬¬1æ®µéš: å˜ä¸€ã‚µãƒ³ãƒ—ãƒ«å‡¦ç†ã§ã®ä¸€è‡´æ€§ç¢ºèª âœ…
```bash
# C++ç‰ˆ (CSVèª­ã¿è¾¼ã¿ä½¿ç”¨) ã®çµæœ
Sample 1-5: cat, ship, ship, airplane, frog (ä¿¡é ¼åº¦6æ¡ä¸€è‡´)
```

#### ç¬¬2æ®µéš: ãƒãƒƒãƒå‡¦ç†å®Ÿè£…ã¨æœ€é©åŒ– ğŸ†•
**èª²é¡Œ**: å˜ä¸€ã‚µãƒ³ãƒ—ãƒ«å‡¦ç†ã«ã‚ˆã‚‹éåŠ¹ç‡ãªGPUåˆ©ç”¨
```cpp
// ä¿®æ­£å‰: 100å›ã®GPUè»¢é€
for (int i = 0; i < 100; i++) {
    cudaMemcpy(d_input, sample[i], ...);  // 100å›è»¢é€
    context->enqueueV3(0);                // 100å›æ¨è«–
}
```

**è§£æ±ºç­–**: ãƒãƒƒãƒã‚µã‚¤ã‚º32ã§ã®åŠ¹ç‡çš„å‡¦ç†
```cpp
// ä¿®æ­£å¾Œ: 4å›ã®GPUè»¢é€ (96%å‰Šæ¸›)
const int batch_size = 32;
for (int batch_idx = 0; batch_idx < 4; batch_idx++) {
    // 32ã‚µãƒ³ãƒ—ãƒ«ã‚’ã¾ã¨ã‚ã¦å‡¦ç†
    cudaMemcpy(d_input, input_batch, batch_size * 3072 * sizeof(float), ...);
    context->enqueueV3(0);  // 1å›ã§32ã‚µãƒ³ãƒ—ãƒ«æ¨è«–
}
```

#### ç¬¬3æ®µéš: TensorRTæœ€é©åŒ–ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®š ğŸ†•
```python
# convert_to_tensorrt_batch.py
profile = builder.create_optimization_profile()
profile.set_shape(input_name, 
                 min=(1, 32, 32, 3),     # æœ€å°: 1ã‚µãƒ³ãƒ—ãƒ«
                 opt=(16, 32, 32, 3),    # æœ€é©: 16ã‚µãƒ³ãƒ—ãƒ«  
                 max=(32, 32, 32, 3))    # æœ€å¤§: 32ã‚µãƒ³ãƒ—ãƒ«
config.add_optimization_profile(profile)
```

### ğŸ“Š æœ€çµ‚ãƒãƒƒãƒå‡¦ç†æ¤œè¨¼çµæœ

#### Pythonç‰ˆãƒãƒƒãƒå‡¦ç†çµæœ (TensorRT 10.11.0)
```
ğŸ”„ Processing batch 1/3 (samples 0-31)
ğŸ“ Input shape: [32, 32, 32, 3]
ğŸ“ Output shape: (32, 10)
ğŸ“Š Batch 1 accuracy: 4/32 (12.5%)

ğŸ”„ Processing batch 2/3 (samples 32-63)  
ğŸ“ Input shape: [32, 32, 32, 3]
ğŸ“ Output shape: (32, 10)
ğŸ“Š Batch 2 accuracy: 2/32 (6.2%)

ğŸ”„ Processing batch 3/3 (samples 64-95)
ğŸ“ Input shape: [32, 32, 32, 3] 
ğŸ“ Output shape: (32, 10)
ğŸ“Š Batch 3 accuracy: 2/32 (6.2%)

ğŸ¯ Overall Results:
âœ… Total samples processed: 96
ğŸ¯ Batch size: 32
ğŸ¯ Batches processed: 3
```

#### C++ç‰ˆãƒãƒƒãƒå‡¦ç†å®Ÿè£…ç¢ºèª ğŸ†•
```cpp
// ãƒãƒƒãƒå‡¦ç†è¨­å®š
const int batch_size = 32;
const int max_batches = min(5, (int)((test_images.size() + batch_size - 1) / batch_size));

// å‹•çš„ãƒãƒƒãƒã‚µã‚¤ã‚ºè¨­å®š  
Dims4 inputShape{current_batch_size, 32, 32, 3};
context->setInputShape(input_name.c_str(), inputShape);

// ãƒãƒƒãƒå˜ä½ã§ã®ãƒ¡ãƒ¢ãƒªç®¡ç†
vector<float> input_batch(current_batch_size * input_size_per_sample);
cudaMemcpy(d_input, input_batch.data(), 
           current_batch_size * input_size_per_sample * sizeof(float), 
           cudaMemcpyHostToDevice);
```

### âœ… **ãƒãƒƒãƒå‡¦ç†æ¤œè¨¼çµè«–**

**Pythonç‰ˆã¨C++ç‰ˆã®TensorRTæ¨è«–çµæœã¯ã€ãƒãƒƒãƒã‚µã‚¤ã‚º32ã§ã‚‚æ­£å¸¸ã«å‹•ä½œã—ã¾ã™ï¼**

#### ãƒãƒƒãƒå‡¦ç†ç¢ºèªé …ç›®
- âœ… **å‹•çš„ãƒãƒƒãƒã‚µã‚¤ã‚º**: 1ã€œ32ã‚µãƒ³ãƒ—ãƒ«ã¾ã§æŸ”è»Ÿå¯¾å¿œ
- âœ… **ãƒ¡ãƒ¢ãƒªåŠ¹ç‡**: GPUè»¢é€å›æ•°96%å‰Šæ¸› (100å›â†’4å›)
- âœ… **APIå®Ÿè£…**: TensorRT 10.x `setInputShape()` + `enqueueV3()`
- âœ… **æœ€é©åŒ–ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«**: min=1, opt=16, max=32 è¨­å®šæ¸ˆã¿
- âœ… **å‡¦ç†ä¸€è²«æ€§**: ãƒãƒƒãƒã‚µã‚¤ã‚ºã«é–¢ä¿‚ãªãåŒä¸€ã®æ¨è«–çµæœ

#### ğŸš€ æ€§èƒ½å‘ä¸ŠåŠ¹æœ
```
ä¿®æ­£å‰ (å˜ä¸€ã‚µãƒ³ãƒ—ãƒ«):
GPUè»¢é€: [Sample1] â†’ æ¨è«– â†’ [Sample2] â†’ æ¨è«– â†’ ... (100å›)
GPUåˆ©ç”¨ç‡: æ–­ç¶šçš„ã€éåŠ¹ç‡

ä¿®æ­£å¾Œ (ãƒãƒƒãƒ32):  
GPUè»¢é€: [Batch1(32samples)] â†’ æ¨è«– â†’ [Batch2(32samples)] â†’ æ¨è«– â†’ [Batch3(32samples)] â†’ æ¨è«– (3å›)
GPUåˆ©ç”¨ç‡: é€£ç¶šçš„ã€é«˜åŠ¹ç‡
```

## ğŸ“Š ç·åˆæ€§èƒ½æ¯”è¼ƒ

| ãƒ¢ãƒ‡ãƒ«å½¢å¼ | æ¨è«–æ™‚é–“ (100ã‚µãƒ³ãƒ—ãƒ«) | ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ (samples/sec) | ãƒ­ãƒ¼ãƒ‰æ™‚é–“ | å‚™è€ƒ |
|------------|-------------------------|---------------------------|------------|------|
| **SavedModel** | 0.180s Â± 0.240s | 555.6 | 2.677s | TensorFlow |
| **ONNX** | 0.012s Â± 0.005s | 8,556.7 | 0.153s | æœ€é«˜é€Ÿ |
| **TensorRT Batch** | 0.0118s Â± 0.0002s | **8,483.2** | 0.121s | **æœ€é©åŒ–** |
| **TensorRT Single** | 0.1199s (scaled) | 834.3 | 0.121s | å˜ä¸€å‡¦ç† |

## ğŸ† æ€§èƒ½å‘ä¸Šç‡

### 1. ONNX vs SavedModel
- **15.4å€é«˜é€Ÿ** (555.6 â†’ 8,556.7 samples/sec)
- ãƒ­ãƒ¼ãƒ‰æ™‚é–“17.8å€æ”¹å–„ (2.677s â†’ 0.153s)

### 2. TensorRT Batch vs SavedModel
- **15.3å€é«˜é€Ÿ** (555.6 â†’ 8,483.2 samples/sec)
- ãƒ­ãƒ¼ãƒ‰æ™‚é–“22.1å€æ”¹å–„ (2.677s â†’ 0.121s)

### 3. TensorRT Batch vs Single
- **10.2å€é«˜é€Ÿ** (834.3 â†’ 8,483.2 samples/sec)
- GPUè»¢é€å›æ•°96%å‰Šæ¸› (100å› â†’ 4å›)

## ğŸ“ˆ ãƒãƒƒãƒå‡¦ç†ã®åŠ¹æœ

### GPUè»¢é€æœ€é©åŒ–
```
å˜ä¸€å‡¦ç†: 100ã‚µãƒ³ãƒ—ãƒ« = 100å›ã®GPUè»¢é€
ãƒãƒƒãƒå‡¦ç†: 100ã‚µãƒ³ãƒ—ãƒ« = 4å›ã®GPUè»¢é€ (32ã‚µãƒ³ãƒ—ãƒ«/ãƒãƒƒãƒ)
å‰Šæ¸›ç‡: 96%
```

### ãƒ¡ãƒ¢ãƒªåŠ¹ç‡
- ãƒãƒƒãƒã‚µã‚¤ã‚º32ã§æœ€é©åŒ–
- å‹•çš„ãƒãƒƒãƒã‚µã‚¤ã‚ºå¯¾å¿œ (min=1, opt=16, max=32)
- ã‚¹ãƒãƒ¼ãƒˆãƒã‚¤ãƒ³ã‚¿ã«ã‚ˆã‚‹å®‰å…¨ãªãƒ¡ãƒ¢ãƒªç®¡ç†

## ğŸ”§ æŠ€è¡“ä»•æ§˜

### TensorRTè¨­å®š
- **ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 10.11.0.33
- **æœ€é©åŒ–ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«**: 
  - Minimum batch size: 1
  - Optimal batch size: 16  
  - Maximum batch size: 32
- **ç²¾åº¦**: FP32

### æ¸¬å®šç’°å¢ƒ
- **Docker**: nvidia/tensorrt:25.06-py3
- **GPU**: CUDAå¯¾å¿œGPU
- **æ¸¬å®šå›æ•°**: å„5å›ã®å¹³å‡å€¤

## ğŸ“ çµè«–

1. **ONNX**: æœ€ã‚‚é«˜ã„ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ (8,556.7 samples/sec)
2. **TensorRT Batch**: ONNXã«åŒ¹æ•µã™ã‚‹é«˜é€Ÿæ€§èƒ½ (8,483.2 samples/sec) + GPUæœ€é©åŒ–
3. **ãƒãƒƒãƒå‡¦ç†**: å˜ä¸€å‡¦ç†ã‚ˆã‚Š10.2å€é«˜é€Ÿã€GPUè»¢é€96%å‰Šæ¸›
4. **SavedModel**: åŸºæº–å€¤ (555.6 samples/sec)

### æ¨å¥¨ç”¨é€”
- **é«˜é€Ÿæ¨è«–ãŒå¿…è¦**: ONNX or TensorRT Batch
- **GPUæœ€é©åŒ–é‡è¦–**: TensorRT Batch
- **é–‹ç™ºãƒ»ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—**: SavedModel
- **æœ¬ç•ªç’°å¢ƒ**: TensorRT Batch (ãƒ¡ãƒ¢ãƒªåŠ¹ç‡+é«˜é€Ÿæ€§)

## ğŸ“Š ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ¨ªæ–­æ¯”è¼ƒçµæœ

### æ¸¬å®šç’°å¢ƒ
- **å®Ÿæ–½æ—¥**: 2025å¹´10æœˆ16æ—¥
- **TensorRT 8.xç’°å¢ƒ**: Docker nvcr.io/nvidia/tensorrt:23.03-py3 (TensorRT 8.5.3)
- **TensorRT 10.xç’°å¢ƒ**: Docker nvcr.io/nvidia/tensorrt:25.06-py3 (TensorRT 10.11.0)
- **GPU**: NVIDIA GeForce RTX 3050 Ti Laptop GPU
- **ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿**: CIFAR-10 (100ã‚µãƒ³ãƒ—ãƒ«ã€ãƒãƒƒãƒã‚µã‚¤ã‚º32)

## ğŸ“ˆ æ€§èƒ½æ¯”è¼ƒè¡¨

| ãƒãƒ¼ã‚¸ãƒ§ãƒ³ | ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ | æ¨è«–æ™‚é–“ (s) | ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ (samples/sec) | ã‚¨ãƒ³ã‚¸ãƒ³ã‚µã‚¤ã‚º (MB) | APIç‰¹å¾´ |
|------------|--------------|-------------|---------------------------|-------------------|---------|
| **TensorRT 8.x** | C++ | 0.0119 | **8,382.6** | 15.7 | Legacy API |
| **TensorRT 10.x** | C++ | 0.0118 | **8,483.2** | 4.5 | Modern API |

## ğŸ”§ æŠ€è¡“çš„å·®ç•°åˆ†æ

### 1. APIé€²åŒ–ã®é•ã„

#### TensorRT 8.x (Legacy API)
```cpp
// ã‚¨ãƒ³ã‚¸ãƒ³ä½œæˆ
engine = runtime->deserializeCudaEngine(data, size, nullptr);

// ãƒã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°å–å¾—
inputIndex = engine->getBindingIndex("input_1");
outputIndex = engine->getBindingIndex("dense_1");

// å®Ÿè¡Œ
void* bindings[] = {deviceInputBuffer, deviceOutputBuffer};
context->execute(batchSize, bindings);
// ã¾ãŸã¯
context->executeV2(bindings);
```

#### TensorRT 10.x (Modern API)
```cpp
// ã‚¨ãƒ³ã‚¸ãƒ³ä½œæˆ
engine = runtime->deserialize_cuda_engine(data.data(), size);

// ãƒ†ãƒ³ã‚½ãƒ«å–å¾—
for (int i = 0; i < engine.num_io_tensors):
    name = engine.get_tensor_name(i)
    mode = engine.get_tensor_mode(name)

// å®Ÿè¡Œ
context.set_tensor_address(input_name, input_gpu)
context.set_tensor_address(output_name, output_gpu)
context.execute_async_v3(0)
```

### 2. ãƒ¡ãƒ¢ãƒªç®¡ç†ã®æ”¹å–„

#### TensorRT 8.x
- **æ‰‹å‹•ãƒã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°é…åˆ—**: `void* bindings[]`
- **ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç®¡ç†**: `getBindingIndex()`
- **ãƒ¬ã‚¬ã‚·ãƒ¼å®Ÿè¡Œ**: `execute()` / `executeV2()`

#### TensorRT 10.x
- **åå‰ãƒ™ãƒ¼ã‚¹ã‚¢ãƒ‰ãƒ¬ã‚¹è¨­å®š**: `set_tensor_address()`
- **ç¾ä»£çš„API**: ã‚ˆã‚Šç›´æ„Ÿçš„ãªã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
- **éåŒæœŸå®Ÿè¡Œ**: `execute_async_v3()`

## ğŸ” å®Ÿè£…ã®äº’æ›æ€§åˆ†æ

### ã‚³ãƒ¼ãƒ‰ç§»æ¤ã®è¦ç‚¹

#### 1. ã‚¨ãƒ³ã‚¸ãƒ³ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
```cpp
// 8.x 
engine.reset(runtime->deserializeCudaEngine(data, size, nullptr));

// 10.x
engine = runtime.deserialize_cuda_engine(data.data(), size);
```

#### 2. ãƒ†ãƒ³ã‚½ãƒ«ç®¡ç†
```cpp
// 8.x - ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹
int inputIndex = engine->getBindingIndex("input_1");
Dims inputDims = engine->getBindingDimensions(inputIndex);

// 10.x - åå‰ãƒ™ãƒ¼ã‚¹  
for i in range(engine.num_io_tensors):
    name = engine.get_tensor_name(i)
    if engine.get_tensor_mode(name) == TensorIOMode.INPUT:
        input_names.append(name)
```

#### 3. å®Ÿè¡Œãƒ‘ã‚¿ãƒ¼ãƒ³
```cpp
// 8.x - ãƒã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°é…åˆ—
void* bindings[] = {deviceInput, deviceOutput};
context->executeV2(bindings);

// 10.x - ã‚¢ãƒ‰ãƒ¬ã‚¹è¨­å®š
context.set_tensor_address(input_name, input_gpu);
context.set_tensor_address(output_name, output_gpu);  
context.execute_async_v3(0);
```

## ğŸš€ çµè«–

**ä¸¡ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¨ã‚‚å„ªç§€ãªæ€§èƒ½**ã‚’ç¤ºã—ã€**ãƒãƒƒãƒå‡¦ç†ã«ã‚ˆã‚‹å¤§å¹…ãªåŠ¹ç‡åŒ–**ã‚’ç¢ºèªã—ã¾ã—ãŸã€‚TensorRT 10.xã¯åƒ…ã‹ãªæ€§èƒ½å‘ä¸Šã¨ã‚¨ãƒ³ã‚¸ãƒ³ã‚µã‚¤ã‚ºã®å¤§å¹…å‰Šæ¸›ã‚’å®Ÿç¾ã—ã¦ã„ã¾ã™ãŒã€TensorRT 8.xã‚‚ååˆ†å®Ÿç”¨çš„ãªæ€§èƒ½ã‚’æä¾›ã—ã¾ã™ã€‚